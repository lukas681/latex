
@incollection{chen_differentially_2023,
	series = {Proceedings},
	title = {Differentially {Private} {All}-{Pairs} {Shortest} {Path} {Distances}: {Improved} {Algorithms} and {Lower} {Bounds}},
	shorttitle = {Differentially {Private} {All}-{Pairs} {Shortest} {Path} {Distances}},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611977554.ch184},
	abstract = {We study the problem of releasing the weights of all-pairs shortest paths in a weighted undirected graph with differential privacy (DP). In this setting, the underlying graph is fixed and two graphs are neighbors if their edge weights differ by at most 1 in the ℓ1-distance. We give an algorithm with additive error Õ(n2/3/ε) in the ε-DP case and an algorithm with additive error  in the (ε,δ)-DP case, where n denotes the number of vertices. This positively answers a question of Sealfon [Sea16, Sea20], who asked whether a o(n)- error algorithm exists. We also show that an additive error of Ω(n1/6) is necessary for any sufficiently small ε,δ {\textgreater} 0.Furthermore, we show that if the graph is promised to have reasonably bounded weights, one can improve the error further to roughly  in the ε-DP case and roughly  in the (ε, δ)-DP case. Previously, it was only known how to obtain Õ(n2/3/ε1/3) additive error in the ε-DP case and  additive error in the (ε,δ)-DP case for bounded-weight graphs [Sea16].Finally, we consider a relaxation where a multiplicative approximation is allowed. We show that, with a multiplicative approximation factor k, the additive error can be reduced to Õ(n1/2+O(1/k)/ε) in the ε-DP case and Õ(n1/3+O(1/k)/ε) in the (ε,δ)-DP case.},
	urldate = {2024-10-30},
	booktitle = {Proceedings of the 2023 {Annual} {ACM}-{SIAM} {Symposium} on {Discrete} {Algorithms} ({SODA})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Chen, Justin Y. and Ghazi, Badih and Kumar, Ravi and Manurangsi, Pasin and Narayanan, Shyam and Nelson, Jelani and Xu, Yinzhan},
	month = jan,
	year = {2023},
	doi = {10.1137/1.9781611977554.ch184},
	pages = {5040--5067},
}

@misc{pan_randomized_2024,
	title = {Randomized {Response} with {Gradual} {Release} of {Privacy} {Budget}},
	url = {http://arxiv.org/abs/2401.13952},
	doi = {10.48550/arXiv.2401.13952},
	abstract = {An algorithm is developed to gradually relax the Differential Privacy (DP) guarantee of a randomized response. The output from each relaxation maintains the same probability distribution as a standard randomized response with the equivalent DP guarantee, ensuring identical utility as the standard approach. The entire relaxation process is proven to have the same DP guarantee as the most recent relaxed guarantee. The DP relaxation algorithm is adaptable to any Local Differential Privacy (LDP) mechanisms relying on randomized response. It has been seamlessly integrated into RAPPOR, an LDP crowdsourcing string-collecting tool, to optimize the utility of estimating the frequency of collected data. Additionally, it facilitates the relaxation of the DP guarantee for mean estimation based on randomized response. Finally, numerical experiments have been conducted to validate the utility and DP guarantee of the algorithm.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Pan, Mingen},
	month = jan,
	year = {2024},
	note = {arXiv:2401.13952},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{ghazi_differential_2024,
	title = {Differential {Privacy} on {Trust} {Graphs}},
	url = {http://arxiv.org/abs/2410.12045},
	doi = {10.48550/arXiv.2410.12045},
	abstract = {We study differential privacy (DP) in a multi-party setting where each party only trusts a (known) subset of the other parties with its data. Specifically, given a trust graph where vertices correspond to parties and neighbors are mutually trusting, we give a DP algorithm for aggregation with a much better privacy-utility trade-off than in the well-studied local model of DP (where each party trusts no other party). We further study a robust variant where each party trusts all but an unknown subset of at most \$t\$ of its neighbors (where \$t\$ is a given parameter), and give an algorithm for this setting. We complement our algorithms with lower bounds, and discuss implications of our work to other tasks in private learning and analytics.},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Ghazi, Badih and Kumar, Ravi and Manurangsi, Pasin and Wang, Serena},
	month = oct,
	year = {2024},
	note = {arXiv:2410.12045},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
}

@misc{sealfon_shortest_2016,
	title = {Shortest {Paths} and {Distances} with {Differential} {Privacy}},
	url = {http://arxiv.org/abs/1511.04631},
	doi = {10.48550/arXiv.1511.04631},
	abstract = {We introduce a model for differentially private analysis of weighted graphs in which the graph topology \$(V,E)\$ is assumed to be public and the private information consists only of the edge weights \$w:E{\textbackslash}to{\textbackslash}mathbb\{R\}{\textasciicircum}+\$. This can express hiding congestion patterns in a known system of roads. Differential privacy requires that the output of an algorithm provides little advantage, measured by privacy parameters \${\textbackslash}epsilon\$ and \${\textbackslash}delta\$, for distinguishing between neighboring inputs, which are thought of as inputs that differ on the contribution of one individual. In our model, two weight functions \$w,w'\$ are considered to be neighboring if they have \${\textbackslash}ell\_1\$ distance at most one. We study the problems of privately releasing a short path between a pair of vertices and of privately releasing approximate distances between all pairs of vertices. We are concerned with the approximation error, the difference between the length of the released path or released distance and the length of the shortest path or actual distance. For privately releasing a short path between a pair of vertices, we prove a lower bound of \${\textbackslash}Omega({\textbar}V{\textbar})\$ on the additive approximation error for fixed \${\textbackslash}epsilon,{\textbackslash}delta\$. We provide a differentially private algorithm that matches this error bound up to a logarithmic factor and releases paths between all pairs of vertices. The approximation error of our algorithm can be bounded by the number of edges on the shortest path, so we achieve better accuracy than the worst-case bound for vertex pairs that are connected by a low-weight path with \$o({\textbar}V{\textbar})\$ vertices. For privately releasing all-pairs distances, we show that for trees we can release all distances with approximation error \$O({\textbackslash}log{\textasciicircum}\{2.5\}{\textbar}V{\textbar})\$ for fixed privacy parameters. For arbitrary bounded-weight graphs with edge weights in \$[0,M]\$ we can release all distances with approximation error \${\textbackslash}tilde\{O\}({\textbackslash}sqrt\{{\textbar}V{\textbar}M\})\$.},
	urldate = {2024-07-03},
	publisher = {arXiv},
	author = {Sealfon, Adam},
	month = apr,
	year = {2016},
	note = {arXiv:1511.04631 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms},
}

@article{author_optimal_nodate,
	title = {Optimal {Private} {Minimum} {Spanning} {Trees} {Under} \_},
	volume = {1},
	abstract = {CCS Concepts: • Do Not Use This Code → Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper; Generate the Correct Terms for Your Paper.},
	language = {en},
	number = {1},
	author = {Author, Anonymous},
}

@misc{noauthor_240115781_nodate,
	title = {[2401.15781] {The} {Discrepancy} of {Shortest} {Paths}},
	url = {https://arxiv.org/abs/2401.15781},
	urldate = {2024-10-14},
}

@misc{noauthor_differential-privacycommon_docsdelta_for_thresholdingpdf_nodate,
	title = {differential-privacy/common\_docs/{Delta}\_For\_Thresholding.pdf at main · google/differential-privacy},
	url = {https://github.com/google/differential-privacy/blob/main/common_docs/Delta_For_Thresholding.pdf},
	abstract = {Google's differential privacy libraries. Contribute to google/differential-privacy development by creating an account on GitHub.},
	language = {en},
	urldate = {2024-10-08},
	journal = {GitHub},
}

@misc{gillenwater_joint_2022,
	title = {A {Joint} {Exponential} {Mechanism} {For} {Differentially} {Private} {Top}-\$k\$},
	url = {http://arxiv.org/abs/2201.12333},
	abstract = {We present a diﬀerentially private algorithm for releasing the sequence of k elements with the highest counts from a data domain of d elements. The algorithm is a “joint” instance of the exponential mechanism, and its output space consists of all O(dk) length-k sequences. Our main contribution is a method to sample this exponential mechanism in time O(dk log(k) + d log(d)) and space O(dk). Experiments show that this approach outperforms existing pure diﬀerential privacy methods and improves upon even approximate diﬀerential privacy methods for moderate k.},
	language = {en},
	urldate = {2024-10-03},
	publisher = {arXiv},
	author = {Gillenwater, Jennifer and Joseph, Matthew and Medina, Andrés Muñoz and Ribero, Mónica},
	month = aug,
	year = {2022},
	note = {arXiv:2201.12333 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@article{akob_laage-thomsen_kunstig_nodate,
	title = {Kunstig intelligens i den offentlige  forvaltning: sammenhænge mellem  algoritmisk regulering og automatisering  af beslutninger i de danske {AI}  ”signaturprojekter”},
	author = {akob Laage-Thomsen},
}

@misc{durfee_practical_2019,
	title = {Practical {Differentially} {Private} {Top}-\$k\$ {Selection} with {Pay}-what-you-get {Composition}},
	url = {http://arxiv.org/abs/1905.04273},
	abstract = {We study the problem of top-k selection over a large domain universe subject to user-level diﬀerential privacy. Typically, the exponential mechanism or report noisy max are the algorithms used to solve this problem. However, these algorithms require querying the database for the count of each domain element. We focus on the setting where the data domain is unknown, which is diﬀerent than the setting of frequent itemsets where an apriori type algorithm can help prune the space of domain elements to query. We design algorithms that ensures (approximate) (ε, δ {\textgreater} 0)-diﬀerential privacy and only needs access to the true top-k¯ elements from the data for any chosen k¯ ≥ k. This is a highly desirable feature for making diﬀerential privacy practical, since the algorithms require no knowledge of the domain. We consider both the setting where a user’s data can modify an arbitrary number of counts by at most 1, i.e. unrestricted sensitivity, and the setting where a user’s data can modify at most some small, ﬁxed number of counts by at most 1, i.e. restricted sensitivity. Additionally, we provide a pay-what-you-get privacy composition bound for our algorithms. That is, our algorithms might return fewer than k elements when the top-k elements are queried, but the overall privacy budget only decreases by the size of the outcome set.},
	language = {en},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Durfee, David and Rogers, Ryan},
	month = sep,
	year = {2019},
	note = {arXiv:1905.04273 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{durfee_practical_2019-1,
	title = {Practical {Differentially} {Private} {Top}-\$k\$ {Selection} with {Pay}-what-you-get {Composition}},
	url = {http://arxiv.org/abs/1905.04273},
	doi = {10.48550/arXiv.1905.04273},
	abstract = {We study the problem of top-\$k\$ selection over a large domain universe subject to user-level differential privacy. Typically, the exponential mechanism or report noisy max are the algorithms used to solve this problem. However, these algorithms require querying the database for the count of each domain element. We focus on the setting where the data domain is unknown, which is different than the setting of frequent itemsets where an apriori type algorithm can help prune the space of domain elements to query. We design algorithms that ensures (approximate) \$({\textbackslash}epsilon,{\textbackslash}delta{\textgreater}0)\$-differential privacy and only needs access to the true top-\${\textbackslash}bar\{k\}\$ elements from the data for any chosen \${\textbackslash}bar\{k\} {\textbackslash}geq k\$. This is a highly desirable feature for making differential privacy practical, since the algorithms require no knowledge of the domain. We consider both the setting where a user's data can modify an arbitrary number of counts by at most 1, i.e. unrestricted sensitivity, and the setting where a user's data can modify at most some small, fixed number of counts by at most 1, i.e. restricted sensitivity. Additionally, we provide a pay-what-you-get privacy composition bound for our algorithms. That is, our algorithms might return fewer than \$k\$ elements when the top-\$k\$ elements are queried, but the overall privacy budget only decreases by the size of the outcome set.},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Durfee, David and Rogers, Ryan},
	month = sep,
	year = {2019},
	note = {arXiv:1905.04273 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@inproceedings{durfee_practical_2019-2,
	title = {Practical {Differentially} {Private} {Top}-k {Selection} with {Pay}-what-you-get {Composition}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/b139e104214a08ae3f2ebcce149cdf6e-Abstract.html},
	abstract = {We study the problem of top-k selection over a large domain universe subject to user-level differential privacy.  Typically, the exponential mechanism or report noisy max are the algorithms used to solve this problem.  However, these algorithms require querying the database for the count of each domain element.  We focus on the setting where the data domain is unknown, which is different than the setting of frequent itemsets where an apriori type algorithm can help prune the space of domain elements to query.  We design algorithms that ensures (approximate) differential privacy and only needs access to the true top-k' elements from the data for any chosen k' ≥ k.  This is a highly desirable feature for making differential privacy practical, since the algorithms require no knowledge of the domain.  We consider both the setting where a user's data can modify an arbitrary number of counts by at most 1, i.e. unrestricted sensitivity, and the setting where a user's data can modify at most some small, fixed number of counts by at most 1, i.e. restricted sensitivity.  Additionally, we provide a pay-what-you-get privacy composition bound for our algorithms.  That is, our algorithms might return fewer than k elements when the top-k elements are queried, but the overall privacy budget only decreases by the size of the outcome set.},
	urldate = {2024-09-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Durfee, David and Rogers, Ryan M},
	year = {2019},
}

@book{cormen_introduction_2022,
	address = {Cambridge, Massachusetts London, England},
	edition = {Fourth edition},
	title = {Introduction to algorithms (4)},
	isbn = {978-0-262-04630-5},
	abstract = {"The leading introductory textbook and reference on algorithms"--},
	language = {eng},
	publisher = {The MIT Press},
	author = {Cormen, Thomas H. and Leiserson, Charles Eric and Rivest, Ronald Linn and Stein, Clifford},
	year = {2022},
}

@book{cormen_introduction_2009,
	address = {Cambridge, Mass},
	edition = {3rd ed},
	title = {Introduction to algorithms (3)},
	isbn = {978-0-262-03384-8 978-0-262-53305-8},
	publisher = {MIT Press},
	editor = {Cormen, Thomas H.},
	year = {2009},
	note = {OCLC: ocn311310321},
	keywords = {Computer algorithms, Computer programming},
}

@book{dwork_algorithmic_2014,
	title = {The {Algorithmic} {Foundations} of {Differential} {Privacy}},
	volume = {9},
	url = {http://dx.doi.org/10.1561/0400000042},
	author = {Dwork, Cynthia and Roth, Aaron},
	year = {2014},
}

@book{blitzstein_introduction_2019,
	address = {Boca Raton},
	edition = {2nd ed},
	series = {Texts in statistical science},
	title = {Introduction to probability},
	isbn = {978-1-138-36991-7},
	abstract = {"Undergraduate probability book that assumes one-semester of calculus. One key is the emphasis on "stories" for the probability distributions (which I mean in both an intuitive and technical sense): there are a dozen or so key distributions (Normal, Binomial, Poisson, etc.) that are incredibly widely-used in statistics, but a lot of books just write down formulas for them without explaining clearly why these particular distributions are so important, or how they are all connected. Each of these distributions has a "story" (a natural application where it arises), and thinking about stories makes the distributions easier to remember, understand, and work with"},
	language = {eng},
	publisher = {CRC press Taylor \& Francis group},
	author = {Blitzstein, Joseph K. and Hwang, Jessica},
	year = {2019},
}

@book{knuth_art_1982,
	address = {Reading, Mass},
	edition = {2. ed., 7. print},
	title = {The art of computer programming.},
	isbn = {978-0-201-03801-9},
	shorttitle = {The art of computer programming. 1},
	publisher = {Addison-Wesley},
	author = {Knuth, Donald Ervin},
	year = {1982},
}

@book{noauthor_art_nodate,
	title = {The {Art} of {Computer} {Programming}},
}

@misc{hoeksma_stochastic_2024,
	title = {Stochastic {Minimum} {Spanning} {Trees} with a {Single} {Sample}},
	url = {http://arxiv.org/abs/2409.16119},
	abstract = {We consider the minimum spanning tree problem in a setting where the edge weights are stochastic from unknown distributions, and the only available information is a single sample of each edge’s weight distribution. In this setting, we analyze the expected performance of the algorithm that outputs a minimum spanning tree for the sampled weights. We compare to the optimal solution when the distributions are known. For every graph with weights that are exponentially distributed, we show that the sampling based algorithm has a performance guarantee that is equal to the size of the largest bond in the graph. Furthermore, we show that for every graph this performance guarantee is tight. The proof is based on two separate inductive arguments via edge contractions, which can be interpreted as reducing the spanning tree problem to a stochastic item selection problem. We also generalize these results to arbitrary matroids, where the performance guarantee is equal to the size of the largest co-circuit of the matroid.},
	language = {en},
	urldate = {2024-09-25},
	publisher = {arXiv},
	author = {Hoeksma, Ruben and Speek, Gavin and Uetz, Marc},
	month = sep,
	year = {2024},
	note = {arXiv:2409.16119 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@misc{bastani_generative_2024,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Generative {AI} {Can} {Harm} {Learning}},
	url = {https://papers.ssrn.com/abstract=4895486},
	doi = {10.2139/ssrn.4895486},
	abstract = {Generative artificial intelligence (AI) is poised to revolutionize how humans work, and has already demonstrated promise in significantly improving human productivity. However, a key remaining question is how generative AI affects learning, namely, how humans acquire new skills as they perform tasks. This kind of skill learning is critical to long-term productivity gains, especially in domains where generative AI is fallible and human experts must check its outputs. We study the impact of generative AI, specifically OpenAI's GPT-4, on human learning in the context of math classes at a high school. In a field experiment involving nearly a thousand students, we have deployed and evaluated two GPT based tutors, one that mimics a standard ChatGPT interface (called GPT Base) and one with prompts designed to safeguard learning (called GPT Tutor). These tutors comprise about 15\% of the curriculum in each of three grades. Consistent with prior work, our results show that access to GPT-4 significantly improves performance (48\% improvement for GPT Base and 127\% for GPT Tutor). However, we additionally find that when access is subsequently taken away, students actually perform worse than those who never had access (17\% reduction for GPT Base). That is, access to GPT-4 can harm educational outcomes. These negative learning effects are largely mitigated by the safeguards included in GPT Tutor. Our results suggest that students attempt to use GPT-4 as a "crutch" during practice problem sessions, and when successful, perform worse on their own. Thus, to maintain long-term productivity, we must be cautious when deploying generative AI to ensure humans continue to learn critical skills.    * HB, OB, and AS contributed equally},
	language = {en},
	urldate = {2024-09-22},
	author = {Bastani, Hamsa and Bastani, Osbert and Sungu, Alp and Ge, Haosen and Kabakcı, Özge and Mariman, Rei},
	month = jul,
	year = {2024},
	keywords = {Education, Generative AI, Human Capital Development, Human-AI Collaboration, Large Language Models},
}

@misc{noauthor_differential-privacycommon_docsdelta_for_thresholdingpdf_nodate-1,
	title = {differential-privacy/common\_docs/{Delta}\_For\_Thresholding.pdf at main · google/differential-privacy},
	url = {https://github.com/google/differential-privacy/blob/main/common_docs/Delta_For_Thresholding.pdf},
	abstract = {Google's differential privacy libraries. Contribute to google/differential-privacy development by creating an account on GitHub.},
	language = {en},
	urldate = {2024-09-19},
	journal = {GitHub},
}

@inproceedings{korolova_releasing_2009,
	address = {New York, NY, USA},
	series = {{WWW} '09},
	title = {Releasing search queries and clicks privately},
	isbn = {978-1-60558-487-4},
	url = {https://doi.org/10.1145/1526709.1526733},
	doi = {10.1145/1526709.1526733},
	abstract = {The question of how to publish an anonymized search log was brought to the forefront by a well-intentioned, but privacy-unaware AOL search log release. Since then a series of ad-hoc techniques have been proposed in the literature, though none are known to be provably private. In this paper, we take a major step towards a solution: we show how queries, clicks and their associated perturbed counts can be published in a manner that rigorously preserves privacy. Our algorithm is decidedly simple to state, but non-trivial to analyze. On the opposite side of privacy is the question of whether the data we can safely publish is of any use. Our findings offer a glimmer of hope: we demonstrate that a non-negligible fraction of queries and clicks can indeed be safely published via a collection of experiments on a real search log. In addition, we select an application, keyword generation, and show that the keyword suggestions generated from the perturbed data resemble those generated from the original data.},
	urldate = {2024-09-19},
	booktitle = {Proceedings of the 18th international conference on {World} wide web},
	publisher = {Association for Computing Machinery},
	author = {Korolova, Aleksandra and Kenthapadi, Krishnaram and Mishra, Nina and Ntoulas, Alexandros},
	month = apr,
	year = {2009},
	pages = {171--180},
}

@misc{nikolov_private_2022,
	title = {Private {Query} {Release} via the {Johnson}-{Lindenstrauss} {Transform}},
	url = {http://arxiv.org/abs/2208.07410},
	doi = {10.48550/arXiv.2208.07410},
	abstract = {We introduce a new method for releasing answers to statistical queries with differential privacy, based on the Johnson-Lindenstrauss lemma. The key idea is to randomly project the query answers to a lower dimensional space so that the distance between any two vectors of feasible query answers is preserved up to an additive error. Then we answer the projected queries using a simple noise-adding mechanism, and lift the answers up to the original dimension. Using this method, we give, for the first time, purely differentially private mechanisms with optimal worst case sample complexity under average error for answering a workload of \$k\$ queries over a universe of size \$N\$. As other applications, we give the first purely private efficient mechanisms with optimal sample complexity for computing the covariance of a bounded high-dimensional distribution, and for answering 2-way marginal queries. We also show that, up to the dependence on the error, a variant of our mechanism is nearly optimal for every given query workload.},
	urldate = {2024-09-17},
	publisher = {arXiv},
	author = {Nikolov, Aleksandar},
	month = aug,
	year = {2022},
	note = {arXiv:2208.07410 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{nikolov_private_2022-1,
	title = {Private {Query} {Release} via the {Johnson}-{Lindenstrauss} {Transform}},
	url = {http://arxiv.org/abs/2208.07410},
	abstract = {We introduce a new method for releasing answers to statistical queries with diﬀerential privacy, based on the Johnson-Lindenstrauss lemma. The key idea is to randomly project the query answers to a lower dimensional space so that the distance between any two vectors of feasible query answers is preserved up to an additive error. Then we answer the projected queries using a simple noise-adding mechanism, and lift the answers up to the original dimension. Using this method, we give, for the ﬁrst time, purely diﬀerentially private mechanisms with optimal worst case sample complexity under average error for answering a workload of k queries over a universe of size N . As other applications, we give the ﬁrst purely private eﬃcient mechanisms with optimal sample complexity for computing the covariance of a bounded high-dimensional distribution, and for answering 2-way marginal queries. We also show that, up to the dependence on the error, a variant of our mechanism is nearly optimal for every given query workload.},
	language = {en},
	urldate = {2024-09-17},
	publisher = {arXiv},
	author = {Nikolov, Aleksandar},
	month = aug,
	year = {2022},
	note = {arXiv:2208.07410 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{steinke_composition_2022,
	title = {Composition of {Differential} {Privacy} \& {Privacy} {Amplification} by {Subsampling}},
	url = {https://arxiv.org/abs/2210.00597v4},
	abstract = {This chapter is meant to be part of the book "Differential Privacy for Artificial Intelligence Applications." We give an introduction to the most important property of differential privacy -- composition: running multiple independent analyses on the data of a set of people will still be differentially private as long as each of the analyses is private on its own -- as well as the related topic of privacy amplification by subsampling. This chapter introduces the basic concepts and gives proofs of the key results needed to apply these tools in practice.},
	language = {no},
	urldate = {2024-09-17},
	journal = {arXiv.org},
	author = {Steinke, Thomas},
	month = oct,
	year = {2022},
}

@article{steinke_between_2017,
	title = {Between {Pure} and {Approximate} {Differential} {Privacy}},
	volume = {7},
	copyright = {http://creativecommons.org/licenses/by-sa/4.0},
	issn = {2575-8527},
	url = {https://journalprivacyconfidentiality.org/index.php/jpc/article/view/648},
	doi = {10.29012/jpc.v7i2.648},
	abstract = {We show a new lower bound on the sample complexity of (ε,δ)-differentially private algorithms that accurately answer statistical queries on high-dimensional databases. The novelty of our bound is that it depends optimally on the parameter δ, which loosely corresponds to the probability that the algorithm fails to be private, and is the first to smoothly interpolate between approximate differential privacy (δ {\textgreater}0) and pure differential privacy (δ= 0). 
  
Specifically, we consider a database D ∈\{±1\}n×d and its one-way marginals, which are the d queries of the form “What fraction of individual records have the i-th bit set to +1?” We show that in order to answer all of these queries to within error ±α (on average) while satisfying (ε,δ)-differential privacy for some function δ such that δ≥2−o(n) and δ≤1/n1+Ω(1), it is necessary that 
 
{\textbackslash}[n≥Ω ({\textbackslash}frac\{√dlog(1/δ)\}\{αε\}).{\textbackslash}] 
 
 This bound is optimal up to constant factors. This lower bound implies similar new bounds for problems like private empirical risk minimization and private PCA. To prove our lower bound, we build on the connection between fingerprinting codes and lower bounds in differential privacy (Bun, Ullman, and Vadhan, STOC’14). 
  
In addition to our lower bound, we give new purely and approximately differentially private algorithms for answering arbitrary statistical queries that improve on the sample complexity of the standard Laplace and Gaussian mechanisms for achieving worst-case accuracy guarantees by a logarithmic factor.},
	language = {en},
	number = {2},
	urldate = {2024-09-16},
	journal = {Journal of Privacy and Confidentiality},
	author = {Steinke, Thomas and Ullman, Jonathan},
	month = jan,
	year = {2017},
}

@misc{bastani_generative_2024-1,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Generative {AI} {Can} {Harm} {Learning}},
	url = {https://papers.ssrn.com/abstract=4895486},
	doi = {10.2139/ssrn.4895486},
	abstract = {Generative artificial intelligence (AI) is poised to revolutionize how humans work, and has already demonstrated promise in significantly improving human productivity. However, a key remaining question is how generative AI affects learning, namely, how humans acquire new skills as they perform tasks. This kind of skill learning is critical to long-term productivity gains, especially in domains where generative AI is fallible and human experts must check its outputs. We study the impact of generative AI, specifically OpenAI's GPT-4, on human learning in the context of math classes at a high school. In a field experiment involving nearly a thousand students, we have deployed and evaluated two GPT based tutors, one that mimics a standard ChatGPT interface (called GPT Base) and one with prompts designed to safeguard learning (called GPT Tutor). These tutors comprise about 15\% of the curriculum in each of three grades. Consistent with prior work, our results show that access to GPT-4 significantly improves performance (48\% improvement for GPT Base and 127\% for GPT Tutor). However, we additionally find that when access is subsequently taken away, students actually perform worse than those who never had access (17\% reduction for GPT Base). That is, access to GPT-4 can harm educational outcomes. These negative learning effects are largely mitigated by the safeguards included in GPT Tutor. Our results suggest that students attempt to use GPT-4 as a "crutch" during practice problem sessions, and when successful, perform worse on their own. Thus, to maintain long-term productivity, we must be cautious when deploying generative AI to ensure humans continue to learn critical skills.    * HB, OB, and AS contributed equally},
	language = {en},
	urldate = {2024-09-14},
	author = {Bastani, Hamsa and Bastani, Osbert and Sungu, Alp and Ge, Haosen and Kabakcı, Özge and Mariman, Rei},
	month = jul,
	year = {2024},
	keywords = {Education, Generative AI, Human Capital Development, Human-AI Collaboration, Large Language Models},
}

@misc{noauthor_sparse_nodate,
	title = {sparse vector technique for l1 - {Google}-søk},
	url = {https://www.google.com/search?q=sparse+vector+technique+for+l1&rlz=1C5GCEM_enDK1092DK1092&oq=sparse+vector+technique+for+l1&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigAdIBCDQ2NDlqMGo3qAIAsAIA&sourceid=chrome&ie=UTF-8},
	urldate = {2024-09-12},
}

@misc{lyu_understanding_2016,
	title = {Understanding the {Sparse} {Vector} {Technique} for {Differential} {Privacy}},
	url = {http://arxiv.org/abs/1603.01699},
	doi = {10.48550/arXiv.1603.01699},
	abstract = {The Sparse Vector Technique (SVT) is a fundamental technique for satisfying differential privacy and has the unique quality that one can output some query answers without apparently paying any privacy cost. SVT has been used in both the interactive setting, where one tries to answer a sequence of queries that are not known ahead of the time, and in the non-interactive setting, where all queries are known. Because of the potential savings on privacy budget, many variants for SVT have been proposed and employed in privacy-preserving data mining and publishing. However, most variants of SVT are actually not private. In this paper, we analyze these errors and identify the misunderstandings that likely contribute to them. We also propose a new version of SVT that provides better utility, and introduce an effective technique to improve the performance of SVT. These enhancements can be applied to improve utility in the interactive setting. Through both analytical and experimental comparisons, we show that, in the non-interactive setting (but not the interactive setting), the SVT technique is unnecessary, as it can be replaced by the Exponential Mechanism (EM) with better accuracy.},
	urldate = {2024-09-11},
	publisher = {arXiv},
	author = {Lyu, Min and Su, Dong and Li, Ninghui},
	month = sep,
	year = {2016},
	note = {arXiv:1603.01699 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{ghazi_avoiding_2020,
	title = {On {Avoiding} the {Union} {Bound} {When} {Answering} {Multiple} {Differentially} {Private} {Queries}},
	url = {http://arxiv.org/abs/2012.09116},
	abstract = {In this work, we study the problem of answering \$k\$ queries with \$({\textbackslash}epsilon, {\textbackslash}delta)\$-differential privacy, where each query has sensitivity one. We give an algorithm for this task that achieves an expected \${\textbackslash}ell\_{\textbackslash}infty\$ error bound of \$O({\textbackslash}frac\{1\}\{{\textbackslash}epsilon\}{\textbackslash}sqrt\{k {\textbackslash}log {\textbackslash}frac\{1\}\{{\textbackslash}delta\}\})\$, which is known to be tight (Steinke and Ullman, 2016). A very recent work by Dagan and Kur (2020) provides a similar result, albeit via a completely different approach. One difference between our work and theirs is that our guarantee holds even when \${\textbackslash}delta {\textless} 2{\textasciicircum}\{-{\textbackslash}Omega(k/({\textbackslash}log k){\textasciicircum}8)\}\$ whereas theirs does not apply in this case. On the other hand, the algorithm of Dagan and Kur has a remarkable advantage that the \${\textbackslash}ell\_\{{\textbackslash}infty\}\$ error bound of \$O({\textbackslash}frac\{1\}\{{\textbackslash}epsilon\}{\textbackslash}sqrt\{k {\textbackslash}log {\textbackslash}frac\{1\}\{{\textbackslash}delta\}\})\$ holds not only in expectation but always (i.e., with probability one) while we can only get a high probability (or expected) guarantee on the error.},
	language = {en},
	urldate = {2024-09-11},
	publisher = {arXiv},
	author = {Ghazi, Badih and Kumar, Ravi and Manurangsi, Pasin},
	month = dec,
	year = {2020},
	note = {arXiv:2012.09116 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
}

@misc{fioretto_differentially_2024,
	title = {Differentially {Private} {Data} {Release} on {Graphs}: {Inefficiencies} and {Unfairness}},
	shorttitle = {Differentially {Private} {Data} {Release} on {Graphs}},
	url = {http://arxiv.org/abs/2408.05246},
	doi = {10.48550/arXiv.2408.05246},
	abstract = {Networks are crucial components of many sectors, including telecommunications, healthcare, finance, energy, and transportation.The information carried in such networks often contains sensitive user data, like location data for commuters and packet data for online users. Therefore, when considering data release for networks, one must ensure that data release mechanisms do not leak information about individuals, quantified in a precise mathematical sense. Differential Privacy (DP) is the widely accepted, formal, state-of-the-art technique, which has found use in a variety of real-life settings including the 2020 U.S. Census, Apple users' device data, or Google's location data. Yet, the use of DP comes with new challenges, as the noise added for privacy introduces inaccuracies or biases and further, DP techniques can also distribute these biases disproportionately across different populations, inducing fairness issues. The goal of this paper is to characterize the impact of DP on bias and unfairness in the context of releasing information about networks, taking a departure from previous work which has studied these effects in the context of private population counts release (such as in the U.S. Census). To this end, we consider a network release problem where the network structure is known to all, but the weights on edges must be released privately. We consider the impact of this private release on a simple downstream decision-making task run by a third-party, which is to find the shortest path between any two pairs of nodes and recommend the best route to users. This setting is of highly practical relevance, mirroring scenarios in transportation networks, where preserving privacy while providing accurate routing information is crucial. Our work provides theoretical foundations and empirical evidence into the bias and unfairness arising due to privacy in these networked decision problems.},
	urldate = {2024-09-10},
	publisher = {arXiv},
	author = {Fioretto, Ferdinando and Sen, Diptangshu and Ziani, Juba},
	month = aug,
	year = {2024},
	note = {arXiv:2408.05246 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{noauthor_differentially_nodate,
	title = {Differentially {Private} {Data} {Release} on {Graphs}: {Inefficiencies} and {Unfairness} - {Google}-søk},
	url = {https://www.google.com/search?q=Differentially+Private+Data+Release+on+Graphs%3A+Inefficiencies+and+Unfairness&rlz=1C5GCEM_enDK1092DK1092&oq=Differentially+Private+Data+Release+on+Graphs%3A+Inefficiencies+and+Unfairness&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBBzg3MmowajSoAgCwAgE&sourceid=chrome&ie=UTF-8},
	urldate = {2024-09-10},
}

@misc{balle_improving_2018,
	title = {Improving the {Gaussian} {Mechanism} for {Differential} {Privacy}: {Analytical} {Calibration} and {Optimal} {Denoising}},
	shorttitle = {Improving the {Gaussian} {Mechanism} for {Differential} {Privacy}},
	url = {http://arxiv.org/abs/1805.06530},
	abstract = {The Gaussian mechanism is an essential building block used in multitude of diﬀerentially private data analysis algorithms. In this paper we revisit the Gaussian mechanism and show that the original analysis has several important limitations. Our analysis reveals that the variance formula for the original mechanism is far from tight in the high privacy regime (ε → 0) and it cannot be extended to the low privacy regime (ε → ∞). We address these limitations by developing an optimal Gaussian mechanism whose variance is calibrated directly using the Gaussian cumulative density function instead of a tail bound approximation. We also propose to equip the Gaussian mechanism with a post-processing step based on adaptive estimation techniques by leveraging that the distribution of the perturbation is known. Our experiments show that analytical calibration removes at least a third of the variance of the noise compared to the classical Gaussian mechanism, and that denoising dramatically improves the accuracy of the Gaussian mechanism in the high-dimensional regime.},
	language = {en},
	urldate = {2024-09-06},
	publisher = {arXiv},
	author = {Balle, Borja and Wang, Yu-Xiang},
	month = jun,
	year = {2018},
	note = {arXiv:1805.06530 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{fung_is_2021,
	title = {Is this the simplest (and most surprising) sorting algorithm ever?},
	url = {http://arxiv.org/abs/2110.01111},
	abstract = {We present an extremely simple sorting algorithm. It may look like it is obviously wrong, but we prove that it is in fact correct. We compare it with other simple sorting algorithms, and analyse some of its curious properties.},
	language = {en},
	urldate = {2024-09-06},
	publisher = {arXiv},
	author = {Fung, Stanley P. Y.},
	month = oct,
	year = {2021},
	note = {arXiv:2110.01111 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms, F.2.2},
}

@article{noauthor_better_nodate,
	title = {Better {Differentially} {Private} {Approximate} {Histograms} and {Heavy} {Hitters} using the {Misra}-{Gries} {Sketch}},
	volume = {1},
	language = {en},
	number = {1},
	journal = {Submission},
}

@misc{karrer_exact_2022,
	title = {Exact {Privacy} {Analysis} of the {Gaussian} {Sparse} {Histogram} {Mechanism}},
	url = {http://arxiv.org/abs/2202.01100},
	abstract = {Sparse histogram methods can be useful for returning differentially private counts of items in large or inﬁnite histograms, large group-by queries, and more generally, releasing a set of statistics with sufﬁcient item counts. We consider the Gaussian version of the sparse histogram mechanism and study the exact , δ differential privacy guarantees satisﬁed by this mechanism. We compare these exact , δ parameters to the simpler overestimates used in prior work to quantify the impact of their looser privacy bounds.},
	language = {en},
	urldate = {2024-09-06},
	publisher = {arXiv},
	author = {Karrer, Brian and Kifer, Daniel and Wilkins, Arjun and Zhang, Danfeng},
	month = feb,
	year = {2022},
	note = {arXiv:2202.01100 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Databases},
}

@misc{karrer_exact_2022-1,
	title = {Exact {Privacy} {Analysis} of the {Gaussian} {Sparse} {Histogram} {Mechanism}},
	url = {http://arxiv.org/abs/2202.01100},
	abstract = {Sparse histogram methods can be useful for returning differentially private counts of items in large or inﬁnite histograms, large group-by queries, and more generally, releasing a set of statistics with sufﬁcient item counts. We consider the Gaussian version of the sparse histogram mechanism and study the exact , δ differential privacy guarantees satisﬁed by this mechanism. We compare these exact , δ parameters to the simpler overestimates used in prior work to quantify the impact of their looser privacy bounds.},
	language = {en},
	urldate = {2024-09-06},
	publisher = {arXiv},
	author = {Karrer, Brian and Kifer, Daniel and Wilkins, Arjun and Zhang, Danfeng},
	month = feb,
	year = {2022},
	note = {arXiv:2202.01100 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Databases},
}

@misc{steinke_between_2015,
	title = {Between {Pure} and {Approximate} {Differential} {Privacy}},
	url = {http://arxiv.org/abs/1501.06095},
	abstract = {We show a new lower bound on the sample complexity of (ε, δ)-diﬀerentially private algorithms that accurately answer statistical queries on high-dimensional databases. The novelty of our bound is that it depends optimally on the parameter δ, which loosely corresponds to the probability that the algorithm fails to be private, and is the ﬁrst to smoothly interpolate between approximate diﬀerential privacy (δ {\textgreater} 0) and pure diﬀerential privacy (δ = 0).},
	language = {en},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Steinke, Thomas and Ullman, Jonathan},
	month = jan,
	year = {2015},
	note = {arXiv:1501.06095 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
}

@inproceedings{dagan_bounded-noise_2022,
	title = {A bounded-noise mechanism for differential privacy},
	url = {https://proceedings.mlr.press/v178/dagan22a.html},
	abstract = {We present an asymptotically optimal \$({\textbackslash}epsilon,{\textbackslash}delta)\$ differentially private mechanism for answering multiple, adaptively asked, \${\textbackslash}Delta\$-sensitive queries, settling the conjecture of Steinke and Ullman [2020].  Our algorithm has a significant advantage that it adds independent bounded noise to each query, thus providing an absolute error bound. Additionally, we apply our algorithm in adaptive data analysis, obtaining an improved guarantee for answering multiple queries regarding some underlying distribution using a finite sample. Numerical computations show that the bounded-noise mechanism outperforms the Gaussian mechanism in many standard settings.},
	language = {en},
	urldate = {2024-08-29},
	booktitle = {Proceedings of {Thirty} {Fifth} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Dagan, Yuval and Kur, Gil},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {625--661},
}

@article{damien_lowering_2020,
	title = {Lowering the cost of anonymization},
	language = {en},
	journal = {PhD Thesis},
	author = {Damien, Desfontaine},
	month = dec,
	year = {2020},
}

@misc{swope_feature_2024,
	title = {Feature {Selection} from {Differentially} {Private} {Correlations}},
	url = {http://arxiv.org/abs/2408.10862},
	doi = {10.48550/arXiv.2408.10862},
	abstract = {Data scientists often seek to identify the most important features in high-dimensional datasets. This can be done through \$L\_1\$-regularized regression, but this can become inefficient for very high-dimensional datasets. Additionally, high-dimensional regression can leak information about individual datapoints in a dataset. In this paper, we empirically evaluate the established baseline method for feature selection with differential privacy, the two-stage selection technique, and show that it is not stable under sparsity. This makes it perform poorly on real-world datasets, so we consider a different approach to private feature selection. We employ a correlations-based order statistic to choose important features from a dataset and privatize them to ensure that the results do not leak information about individual datapoints. We find that our method significantly outperforms the established baseline for private feature selection on many datasets.},
	urldate = {2024-08-27},
	publisher = {arXiv},
	author = {Swope, Ryan and Khanna, Amol and Doldo, Philip and Roy, Saptarshi and Raff, Edward},
	month = aug,
	year = {2024},
	note = {arXiv:2408.10862 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_new_nodate,
	title = {New {Diameter}-{Reducing} {Shortcuts} and {Directed} {Hopsets}: {Breaking} the {Barrier}},
	shorttitle = {New {Diameter}-{Reducing} {Shortcuts} and {Directed} {Hopsets}},
	url = {https://epubs.siam.org/doi/epdf/10.1137/1.9781611977073.55},
	language = {en},
	urldate = {2024-08-26},
	note = {ISBN: 9781611977073},
}

@misc{mckenna_aim_2024,
	title = {{AIM}: {An} {Adaptive} and {Iterative} {Mechanism} for {Differentially} {Private} {Synthetic} {Data}},
	shorttitle = {{AIM}},
	url = {http://arxiv.org/abs/2201.12677},
	doi = {10.48550/arXiv.2201.12677},
	abstract = {We propose AIM, a new algorithm for differentially private synthetic data generation. AIM is a workload-adaptive algorithm within the paradigm of algorithms that first selects a set of queries, then privately measures those queries, and finally generates synthetic data from the noisy measurements. It uses a set of innovative features to iteratively select the most useful measurements, reflecting both their relevance to the workload and their value in approximating the input data. We also provide analytic expressions to bound per-query error with high probability which can be used to construct confidence intervals and inform users about the accuracy of generated data. We show empirically that AIM consistently outperforms a wide variety of existing mechanisms across a variety of experimental settings.},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {McKenna, Ryan and Mullins, Brett and Sheldon, Daniel and Miklau, Gerome},
	month = jun,
	year = {2024},
	note = {arXiv:2201.12677 [cs]},
	keywords = {Computer Science - Databases},
}

@misc{imola_robustness_2022,
	title = {Robustness of {Locally} {Differentially} {Private} {Graph} {Analysis} {Against} {Poisoning}},
	url = {http://arxiv.org/abs/2210.14376},
	doi = {10.48550/arXiv.2210.14376},
	abstract = {Locally differentially private (LDP) graph analysis allows private analysis on a graph that is distributed across multiple users. However, such computations are vulnerable to data poisoning attacks where an adversary can skew the results by submitting malformed data. In this paper, we formally study the impact of poisoning attacks for graph degree estimation protocols under LDP. We make two key technical contributions. First, we observe LDP makes a protocol more vulnerable to poisoning -- the impact of poisoning is worse when the adversary can directly poison their (noisy) responses, rather than their input data. Second, we observe that graph data is naturally redundant -- every edge is shared between two users. Leveraging this data redundancy, we design robust degree estimation protocols under LDP that can significantly reduce the impact of data poisoning and compute degree estimates with high accuracy. We evaluate our proposed robust degree estimation protocols under poisoning attacks on real-world datasets to demonstrate their efficacy in practice.},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Imola, Jacob and Chowdhury, Amrita Roy and Chaudhuri, Kamalika},
	month = oct,
	year = {2022},
	note = {arXiv:2210.14376 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@inproceedings{tramer_position_2024,
	title = {Position: {Considerations} for {Differentially} {Private} {Learning} with {Large}-{Scale} {Public} {Pretraining}},
	shorttitle = {Position},
	url = {https://proceedings.mlr.press/v235/tramer24a.html},
	abstract = {The performance of differentially private machine learning can be boosted significantly by leveraging the transfer learning capabilities of non-private models pretrained on large public datasets. We critically review this approach. We primarily question whether the use of large Web-scraped datasets should be viewed as differential-privacy-preserving. We further scrutinize whether existing machine learning benchmarks are appropriate for measuring the ability of pretrained models to generalize to sensitive domains. Finally, we observe that reliance on large pretrained models may lose other forms of privacy, requiring data to be outsourced to a more compute-powerful third party.},
	language = {en},
	urldate = {2024-08-26},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tramèr, Florian and Kamath, Gautam and Carlini, Nicholas},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {48453--48467},
}

@article{steinke_privacy_nodate,
	title = {Privacy {Auditing} with {One} (1) {Training} {Run}},
	abstract = {We propose a scheme for auditing differentially private machine learning systems with a single training run. This exploits the parallelism of being able to add or remove multiple training examples independently. We analyze this using the connection between differential privacy and statistical generalization, which avoids the cost of group privacy. Our auditing scheme requires minimal assumptions about the algorithm and can be applied in the black-box or white-box setting. We demonstrate the effectiveness of our framework by applying it to DP-SGD, where we can achieve meaningful empirical privacy lower bounds by training only one model. In contrast, standard methods would require training hundreds of models.},
	language = {en},
	author = {Steinke, Thomas and Nasr, Milad and Jagielski, Matthew},
}

@misc{fuentes_joint_2024,
	title = {Joint {Selection}: {Adaptively} {Incorporating} {Public} {Information} for {Private} {Synthetic} {Data}},
	shorttitle = {Joint {Selection}},
	url = {http://arxiv.org/abs/2403.07797},
	abstract = {Mechanisms for generating differentially private synthetic data based on marginals and graphical models have been successful in a wide range of settings. However, one limitation of these methods is their inability to incorporate public data. Initializing a data generating model by pre-training on public data has shown to improve the quality of synthetic data, but this technique is not applicable when model structure is not determined a priori. We develop the mechanism jam-pgm, which expands the adaptive measurements framework to jointly select between measuring public data and private data. This technique allows for public data to be included in a graphical-model-based mechanism. We show that jam-pgm is able to outperform both publicly assisted and non publicly assisted synthetic data generation mechanisms even when the public data distribution is biased.},
	language = {en},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Fuentes, Miguel and Mullins, Brett and McKenna, Ryan and Miklau, Gerome and Sheldon, Daniel},
	month = mar,
	year = {2024},
	note = {arXiv:2403.07797 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{lebeda_better_2024,
	title = {Better {Gaussian} {Mechanism} using {Correlated} {Noise}},
	url = {http://arxiv.org/abs/2408.06853},
	doi = {10.48550/arXiv.2408.06853},
	abstract = {We present a simple variant of the Gaussian mechanism for answering differentially private queries when the sensitivity space has a certain common structure. Our motivating problem is the fundamental task of answering \$d\$ counting queries under the add/remove neighboring relation. The standard Gaussian mechanism solves this task by adding noise distributed as a Gaussian with variance scaled by \$d\$ independently to each count. We show that adding a random variable distributed as a Gaussian with variance scaled by \$({\textbackslash}sqrt\{d\} + 1)/4\$ to all counts allows us to reduce the variance of the independent Gaussian noise samples to scale only with \$(d + {\textbackslash}sqrt\{d\})/4\$. The total noise added to each counting query follows a Gaussian distribution with standard deviation scaled by \$({\textbackslash}sqrt\{d\} + 1)/2\$ rather than \${\textbackslash}sqrt\{d\}\$. The central idea of our mechanism is simple and the technique is flexible. We show that applying our technique to another problem gives similar improvements over the standard Gaussian mechanism.},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Lebeda, Christian Janos},
	month = aug,
	year = {2024},
	note = {arXiv:2408.06853 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms},
}

@misc{dvijotham_efficient_2024,
	title = {Efficient and {Near}-{Optimal} {Noise} {Generation} for {Streaming} {Differential} {Privacy}},
	url = {http://arxiv.org/abs/2404.16706},
	doi = {10.48550/arXiv.2404.16706},
	abstract = {In the task of differentially private (DP) continual counting, we receive a stream of increments and our goal is to output an approximate running total of these increments, without revealing too much about any specific increment. Despite its simplicity, differentially private continual counting has attracted significant attention both in theory and in practice. Existing algorithms for differentially private continual counting are either inefficient in terms of their space usage or add an excessive amount of noise, inducing suboptimal utility. The most practical DP continual counting algorithms add carefully correlated Gaussian noise to the values. The task of choosing the covariance for this noise can be expressed in terms of factoring the lower-triangular matrix of ones (which computes prefix sums). We present two approaches from this class (for different parameter regimes) that achieve near-optimal utility for DP continual counting and only require logarithmic or polylogarithmic space (and time). Our first approach is based on a space-efficient streaming matrix multiplication algorithm for a class of Toeplitz matrices. We show that to instantiate this algorithm for DP continual counting, it is sufficient to find a low-degree rational function that approximates the square root on a circle in the complex plane. We then apply and extend tools from approximation theory to achieve this. We also derive efficient closed-forms for the objective function for arbitrarily many steps, and show direct numerical optimization yields a highly practical solution to the problem. Our second approach combines our first approach with a recursive construction similar to the binary tree mechanism.},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Dvijotham, Krishnamurthy and McMahan, H. Brendan and Pillutla, Krishna and Steinke, Thomas and Thakurta, Abhradeep},
	month = may,
	year = {2024},
	note = {arXiv:2404.16706 [cs]},
	keywords = {Computer Science - Computational Complexity, Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
}

@misc{steinke_privacy_2023,
	title = {Privacy {Auditing} with {One} (1) {Training} {Run}},
	url = {http://arxiv.org/abs/2305.08846},
	abstract = {We propose a scheme for auditing diﬀerentially private machine learning systems with a single training run. This exploits the parallelism of being able to add or remove multiple training examples independently. We analyze this using the connection between diﬀerential privacy and statistical generalization, which avoids the cost of group privacy. Our auditing scheme requires minimal assumptions about the algorithm and can be applied in the black-box or white-box setting.},
	language = {en},
	urldate = {2024-08-26},
	publisher = {arXiv},
	author = {Steinke, Thomas and Nasr, Milad and Jagielski, Matthew},
	month = may,
	year = {2023},
	note = {arXiv:2305.08846 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
}

@inproceedings{koskela_computing_2020,
	title = {Computing {Tight} {Differential} {Privacy} {Guarantees} {Using} {FFT}},
	url = {https://proceedings.mlr.press/v108/koskela20b.html},
	abstract = {Differentially private (DP) machine learning has recently become popular. The privacy loss of DP algorithms is commonly reported using (e.d)-DP. In this paper, we propose a numerical accountant for evaluating the privacy loss for algorithms with continuous one dimensional output. This accountant can be applied to the subsampled multidimensional Gaussian mechanism which underlies the popular DP stochastic gradient descent. The proposed method is based on a numerical approximation of an integral formula which gives the exact (e.d)-values. The approximation is carried out by discretising the integral and by evaluating discrete convolutions using the fast Fourier transform algorithm. We give theoretical error bounds which show the convergence of the approximation and guarantee its accuracy to an arbitrary degree. We give both theoretical error bounds and numerical error estimates for the approximation. Experimental comparisons with state-of-the-art techniques demonstrate significant improvements in bound tightness and/or computation time.},
	language = {en},
	urldate = {2024-08-26},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Koskela, Antti and Jälkö, Joonas and Honkela, Antti},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {2560--2569},
}

@article{chazelle_approximating_2005,
	title = {Approximating the {Minimum} {Spanning} {Tree} {Weight} in {Sublinear} {Time}},
	volume = {34},
	issn = {0097-5397, 1095-7111},
	url = {http://epubs.siam.org/doi/10.1137/S0097539702403244},
	doi = {10.1137/S0097539702403244},
	language = {en},
	number = {6},
	urldate = {2024-08-22},
	journal = {SIAM Journal on Computing},
	author = {Chazelle, Bernard and Rubinfeld, Ronitt and Trevisan, Luca},
	month = jan,
	year = {2005},
	pages = {1370--1379},
}

@misc{campbell_simple_2024,
	title = {A {Simple}, {Nearly}-{Optimal} {Algorithm} for {Differentially} {Private} {All}-{Pairs} {Shortest} {Distances}},
	url = {http://arxiv.org/abs/2407.06913},
	abstract = {The all-pairs shortest distances (APSD) with differential privacy (DP) problem takes as input an undirected, weighted graph \$G = (V,E, {\textbackslash}mathbf\{w\})\$ and outputs a private estimate of the shortest distances in \$G\$ between all pairs of vertices. In this paper, we present a simple \${\textbackslash}widetilde\{O\}(n{\textasciicircum}\{1/3\}/{\textbackslash}varepsilon)\$-accurate algorithm to solve APSD with \${\textbackslash}varepsilon\$-DP, which reduces to \${\textbackslash}widetilde\{O\}(n{\textasciicircum}\{1/4\}/{\textbackslash}varepsilon)\$ in the \$({\textbackslash}varepsilon, {\textbackslash}delta)\$-DP setting, where \$n = {\textbar}V{\textbar}\$. Our algorithm greatly improves upon the error of prior algorithms, namely \${\textbackslash}widetilde\{O\}(n{\textasciicircum}\{2/3\}/{\textbackslash}varepsilon)\$ and \${\textbackslash}widetilde\{O\}({\textbackslash}sqrt\{n\}/{\textbackslash}varepsilon)\$ in the two respective settings, and is the first to be optimal up to a polylogarithmic factor, based on a lower bound of \${\textbackslash}widetilde\{{\textbackslash}Omega\}(n{\textasciicircum}\{1/4\})\$. In the case where a multiplicative approximation is allowed, we give two different constructions of algorithms with reduced additive error. Our first construction allows a multiplicative approximation of \$O(k{\textbackslash}log\{{\textbackslash}log\{n\}\})\$ and has additive error \${\textbackslash}widetilde\{O\}(k{\textbackslash}cdot n{\textasciicircum}\{1/k\}/{\textbackslash}varepsilon)\$ in the \${\textbackslash}varepsilon\$-DP case and \${\textbackslash}widetilde\{O\}({\textbackslash}sqrt\{k\}{\textbackslash}cdot n{\textasciicircum}\{1/(2k)\}/{\textbackslash}varepsilon)\$ in the \$({\textbackslash}varepsilon, {\textbackslash}delta)\$-DP case. Our second construction allows multiplicative approximation \$2k-1\$ and has the same asymptotic additive error as the first construction. Both constructions significantly improve upon the currently best-known additive error of, \${\textbackslash}widetilde\{O\}(k{\textbackslash}cdot n{\textasciicircum}\{1/2 + 1/(4k+2)\}/{\textbackslash}varepsilon)\$ and \${\textbackslash}widetilde\{O\}(k{\textbackslash}cdot n{\textasciicircum}\{1/3 + 2/(9k+3)\}/{\textbackslash}varepsilon)\$, respectively. Our algorithms are straightforward and work by decomposing a graph into a set of spanning trees, and applying a key observation that we can privately release APSD in trees with \$O({\textbackslash}text\{polylog\}(n))\$ error.},
	language = {en},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Campbell, Jesse and Zhu, Chunjiang},
	month = jul,
	year = {2024},
	note = {arXiv:2407.06913 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@misc{joseph_constructions_2024,
	title = {Some {Constructions} of {Private}, {Efficient}, and {Optimal} \${K}\$-{Norm} and {Elliptic} {Gaussian} {Noise}},
	url = {http://arxiv.org/abs/2309.15790},
	abstract = {Differentially private computation often begins with a bound on some d-dimensional statistic’s ℓp sensitivity. For pure differential privacy, the K-norm mechanism can improve on this approach using a norm tailored to the statistic’s sensitivity space. Writing down a closed-form description of this optimal norm is often straightforward. However, running the K-norm mechanism reduces to uniformly sampling the norm’s unit ball; this ball is a d-dimensional convex body, so general sampling algorithms can be slow. Turning to concentrated differential privacy, elliptic Gaussian noise offers similar improvement over spherical Gaussian noise. Once the shape of this ellipse is determined, sampling is easy; however, identifying the best such shape may be hard.},
	language = {en},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Joseph, Matthew and Yu, Alexander},
	month = may,
	year = {2024},
	note = {arXiv:2309.15790 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{chua_how_2024,
	title = {How {Private} are {DP}-{SGD} {Implementations}?},
	url = {http://arxiv.org/abs/2403.17673},
	abstract = {We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ. While shuffling-based DP-SGD is more commonly used in practical implementations, it has not been amenable to easy privacy analysis, either analytically or even numerically. On the other hand, Poisson subsampling-based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available. This has led to a common practice of using shuffling-based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version. Our result shows that there can be a substantial gap between the privacy analysis when using the two types of batch sampling, and thus advises caution in reporting privacy parameters for DP-SGD.},
	language = {en},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Chua, Lynn and Ghazi, Badih and Kamath, Pritish and Kumar, Ravi and Manurangsi, Pasin and Sinha, Amer and Zhang, Chiyuan},
	month = jun,
	year = {2024},
	note = {arXiv:2403.17673 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
}

@misc{nikolov_geometry_2012,
	title = {The {Geometry} of {Differential} {Privacy}: the {Sparse} and {Approximate} {Cases}},
	shorttitle = {The {Geometry} of {Differential} {Privacy}},
	url = {http://arxiv.org/abs/1212.0297},
	doi = {10.1145/2488608.2488652},
	abstract = {In this work, we study trade-oﬀs between accuracy and privacy in the context of linear queries over histograms. This is a rich class of queries that includes contingency tables and range queries, and has been a focus of a long line of work [BLR08,RR10,DRV10,HT10,HR10,LHR+10,BDKT12]. For a given set of d linear queries over a database x ∈ RN , we seek to ﬁnd the diﬀerentially private mechanism that has the minimum mean squared error. For pure diﬀerential privacy, [HT10, BDKT12] give an O(log2 d) approximation to the optimal mechanism. Our ﬁrst contribution is to give an O(log2 d) approximation guarantee for the case of (ε, δ)-diﬀerential privacy. Our mechanism is simple, eﬃcient and adds carefully chosen correlated Gaussian noise to the answers. We prove its approximation guarantee relative to the hereditary discrepancy lower bound of [MN12], using tools from convex geometry.},
	language = {en},
	urldate = {2024-08-20},
	author = {Nikolov, Aleksandar and Talwar, Kunal and Zhang, Li},
	month = dec,
	year = {2012},
	note = {arXiv:1212.0297 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@misc{joseph_constructions_2024-1,
	title = {Some {Constructions} of {Private}, {Efficient}, and {Optimal} \${K}\$-{Norm} and {Elliptic} {Gaussian} {Noise}},
	url = {http://arxiv.org/abs/2309.15790},
	abstract = {Differentially private computation often begins with a bound on some d-dimensional statistic’s ℓp sensitivity. For pure differential privacy, the K-norm mechanism can improve on this approach using a norm tailored to the statistic’s sensitivity space. Writing down a closed-form description of this optimal norm is often straightforward. However, running the K-norm mechanism reduces to uniformly sampling the norm’s unit ball; this ball is a d-dimensional convex body, so general sampling algorithms can be slow. Turning to concentrated differential privacy, elliptic Gaussian noise offers similar improvement over spherical Gaussian noise. Once the shape of this ellipse is determined, sampling is easy; however, identifying the best such shape may be hard.},
	language = {en},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Joseph, Matthew and Yu, Alexander},
	month = may,
	year = {2024},
	note = {arXiv:2309.15790 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{gupta_differentially_2009,
	title = {Differentially {Private} {Combinatorial} {Optimization}},
	url = {http://arxiv.org/abs/0903.4510},
	abstract = {Consider the following problem: given a metric space, some of whose points are "clients", open a set of at most \$k\$ facilities to minimize the average distance from the clients to these facilities. This is just the well-studied \$k\$-median problem, for which many approximation algorithms and hardness results are known. Note that the objective function encourages opening facilities in areas where there are many clients, and given a solution, it is often possible to get a good idea of where the clients are located. However, this poses the following quandary: what if the identity of the clients is sensitive information that we would like to keep private? Is it even possible to design good algorithms for this problem that preserve the privacy of the clients? In this paper, we initiate a systematic study of algorithms for discrete optimization problems in the framework of differential privacy (which formalizes the idea of protecting the privacy of individual input elements). We show that many such problems indeed have good approximation algorithms that preserve differential privacy; this is even in cases where it is impossible to preserve cryptographic definitions of privacy while computing any non-trivial approximation to even the\_value\_ of an optimal solution, let alone the entire solution. Apart from the \$k\$-median problem, we study the problems of vertex and set cover, min-cut, facility location, Steiner tree, and the recently introduced submodular maximization problem, "Combinatorial Public Projects" (CPP).},
	language = {en},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Gupta, Anupam and Ligett, Katrina and McSherry, Frank and Roth, Aaron and Talwar, Kunal},
	month = nov,
	year = {2009},
	note = {arXiv:0903.4510 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms},
}

@article{noauthor_bounding_2018,
	title = {Bounding {Contribution} {Optimally} for {Federated} {Frequency} {Estimation} under {User}-level {Distributed} {Differential} {Privacy}},
	abstract = {We study how to perform federated frequency estimation under user-level distributed differential privacy, where our goal is to protect the privacy of all entries from any single user under secure aggregation protocols. While many works address the fundamental analytics of private frequency estimation under the central and trusted aggregator assumption, this problem has not been specifically addressed under the federated and user-level distributedly private setting. To achieve this, we first introduce a thresholding frequency estimator satisfying the federated and secure aggregation constraints, and show that there is an optimal quantile yielding almost minimal measurement error. Then, we design methods fully compatible with the above-mentioned constraints, to estimate the quantile privately. To this end, we propose end-to-end federated protocols integrating these approaches and assess their performance via extensive experiments. Our evaluations verify our theoretical findings, and demonstrate the effectiveness of the protocols over existing solutions.},
	language = {en},
	year = {2018},
}

@misc{steinke_between_2015-1,
	title = {Between {Pure} and {Approximate} {Differential} {Privacy}},
	url = {http://arxiv.org/abs/1501.06095},
	abstract = {We show a new lower bound on the sample complexity of (ε, δ)-diﬀerentially private algorithms that accurately answer statistical queries on high-dimensional databases. The novelty of our bound is that it depends optimally on the parameter δ, which loosely corresponds to the probability that the algorithm fails to be private, and is the ﬁrst to smoothly interpolate between approximate diﬀerential privacy (δ {\textgreater} 0) and pure diﬀerential privacy (δ = 0).},
	language = {en},
	urldate = {2024-08-16},
	publisher = {arXiv},
	author = {Steinke, Thomas and Ullman, Jonathan},
	month = jan,
	year = {2015},
	note = {arXiv:1501.06095 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
}

@misc{pagh_faster_2024,
	title = {Faster {Private} {Minimum} {Spanning} {Trees}},
	url = {http://arxiv.org/abs/2408.06997},
	doi = {10.48550/arXiv.2408.06997},
	abstract = {Motivated by applications in clustering and synthetic data generation, we consider the problem of releasing a minimum spanning tree (MST) under edge-weight differential privacy constraints where a graph topology \$G=(V,E)\$ with \$n\$ vertices and \$m\$ edges is public, the weight matrix \${\textbackslash}vec\{W\}{\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}\{n {\textbackslash}times n\}\$ is private, and we wish to release an approximate MST under \${\textbackslash}rho\$-zero-concentrated differential privacy. Weight matrices are considered neighboring if they differ by at most \${\textbackslash}Delta\_{\textbackslash}infty\$ in each entry, i.e., we consider an \${\textbackslash}ell\_{\textbackslash}infty\$ neighboring relationship. Existing private MST algorithms either add noise to each entry in \${\textbackslash}vec\{W\}\$ and estimate the MST by post-processing or add noise to weights in-place during the execution of a specific MST algorithm. Using the post-processing approach with an efficient MST algorithm takes \$O(n{\textasciicircum}2)\$ time on dense graphs but results in an additive error on the weight of the MST of magnitude \$O(n{\textasciicircum}2{\textbackslash}log n)\$. In-place algorithms give asymptotically better utility, but the running time of existing in-place algorithms is \$O(n{\textasciicircum}3)\$ for dense graphs. Our main result is a new differentially private MST algorithm that matches the utility of existing in-place methods while running in time \$O(m + n{\textasciicircum}\{3/2\}{\textbackslash}log n)\$ for fixed privacy parameter \${\textbackslash}rho\$. The technical core of our algorithm is an efficient sublinear time simulation of Report-Noisy-Max that works by discretizing all edge weights to a multiple of \${\textbackslash}Delta\_{\textbackslash}infty\$ and forming groups of edges with identical weights. Specifically, we present a data structure that allows us to sample a noisy minimum weight edge among at most \$O(n{\textasciicircum}2)\$ cut edges in \$O({\textbackslash}sqrt\{n\} {\textbackslash}log n)\$ time. Experimental evaluations support our claims that our algorithm significantly improves previous algorithms either in utility or running time.},
	urldate = {2024-08-14},
	publisher = {arXiv},
	author = {Pagh, Rasmus and Retschmeier, Lukas},
	month = aug,
	year = {2024},
	note = {arXiv:2408.06997 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
}

@article{maddalena_8th_nodate,
	title = {8th {International} {Conference} on {Fun} with {Algorithms}},
	language = {en},
	author = {Maddalena, La},
}

@misc{noauthor_privacy_nodate,
	title = {Privacy {Loss} {Ditributions}},
	url = {https://github.com/google/differential-privacy/blob/main/common_docs/Privacy_Loss_Distributions.pdf},
	urldate = {2024-08-12},
}

@article{cai_shortest_2024,
	title = {Shortest {Paths} {Publishing} {With} {Differential} {Privacy}},
	volume = {9},
	issn = {2377-3782},
	url = {https://ieeexplore.ieee.org/document/10308714/?arnumber=10308714},
	doi = {10.1109/TSUSC.2023.3329995},
	abstract = {The growing prevalence of graphs representations in our society has led to a corresponding rise in the publishing of graphs by researchers and organizations. To protect the privacy, it is important to ensure that graphs including sensitive data are not disclosed. Since the weight of edges could be utilized to infer confidential information, the graph should be privately published to avoid ethical and legal issues. In this paper, we propose a novel method for privately publishing shortest paths while preserving the privacy of sensitive edge weights in graph. Specifically, we divide the edge weights into internal and external edges based on their edge betweenness centrality. Then, we give two different differentially private algorithms to perturb edge weights based on the distinction between internal and external edges, respectively. To reduce the error ratios between differentially private shortest paths and real shortest paths, we employ edge betweenness centrality to search for the shortest path, which is closest to the true one. Our experimental results show that our mechanisms can effectively reduce the error in the average shortest path distance by 1.1\% for large graphs, while for the shortest path change rate, our mechanisms can reduce it by 8.3\%.},
	number = {2},
	urldate = {2024-08-09},
	journal = {IEEE Transactions on Sustainable Computing},
	author = {Cai, Bin and Sheng, Weihong and Chen, Jiajun and Hu, Chunqiang and Yu, Jiguo},
	month = mar,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Sustainable Computing},
	keywords = {Cryptography, Differential privacy, Green computing, Privacy, Privacy breach, Publishing, Roads, graph theory, randomized response, shortest path},
	pages = {209--221},
}

@inproceedings{chen_differentially_2015,
	address = {Sydney NSW Australia},
	title = {Differentially {Private} {High}-{Dimensional} {Data} {Publication} via {Sampling}-{Based} {Inference}},
	isbn = {978-1-4503-3664-2},
	url = {https://dl.acm.org/doi/10.1145/2783258.2783379},
	doi = {10.1145/2783258.2783379},
	language = {en},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the 21th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Rui and Xiao, Qian and Zhang, Yu and Xu, Jianliang},
	month = aug,
	year = {2015},
	pages = {129--138},
}

@phdthesis{zhang_algorithms_2016,
	title = {Algorithms for synthetic data release under differential privacy},
	url = {http://hdl.handle.net/10356/69204},
	language = {en},
	urldate = {2024-08-09},
	school = {Nanyang Technological University},
	author = {Zhang, Jun},
	year = {2016},
	doi = {10.32657/10356/69204},
}

@misc{noauthor_anonymization_nodate,
	title = {Anonymization: {The} imperfect science of using data while preserving privacy},
	shorttitle = {Anonymization},
	url = {https://www.science.org/doi/10.1126/sciadv.adn7053},
	language = {en},
	urldate = {2024-08-08},
	doi = {10.1126/sciadv.adn7053},
}

@inproceedings{nelson_efficient_2021,
	title = {Efficient {Error} {Prediction} for {Differentially} {Private} {Algorithms}},
	url = {http://arxiv.org/abs/2103.04816},
	doi = {10.1145/3465481.3465746},
	abstract = {Diﬀerential privacy is a strong mathematical notion of privacy. Still, a prominent challenge when using diﬀerential privacy in real data collection is understanding and counteracting the accuracy loss that diﬀerential privacy imposes. As such, the accuracy/privacy trade-oﬀ of diﬀerential privacy needs to be balanced on a caseby-case basis. Applications in the literature tend to focus solely on analytical accuracy bounds, not include data in error prediction, or use arbitrary settings to measure error empirically.},
	language = {en},
	urldate = {2024-08-05},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	author = {Nelson, Boel},
	month = aug,
	year = {2021},
	note = {arXiv:2103.04816 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	pages = {1--12},
}

@article{caracciolo_random_nodate,
	title = {{THE} {RANDOM} {MINIMUM} {SPANNING} {TREE} {PROBLEM}},
	language = {en},
	author = {Caracciolo, Sergio and Malatesta, Dott Enrico and Riva, Andrea},
}

@inproceedings{cormode_differentially_2012,
	address = {Berlin Germany},
	title = {Differentially private summaries for sparse data},
	isbn = {978-1-4503-0791-8},
	url = {https://dl.acm.org/doi/10.1145/2274576.2274608},
	doi = {10.1145/2274576.2274608},
	language = {en},
	urldate = {2024-08-01},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Database} {Theory}},
	publisher = {ACM},
	author = {Cormode, Graham and Procopiuc, Cecilia and Srivastava, Divesh and Tran, Thanh T. L.},
	month = mar,
	year = {2012},
	pages = {299--311},
}

@misc{cormode_differentially_2011,
	title = {Differentially {Private} {Publication} of {Sparse} {Data}},
	url = {http://arxiv.org/abs/1103.0825},
	abstract = {The problem of privately releasing data is to provide a version of a dataset without revealing sensitive information about the individuals who contribute to the data. The model of differential privacy allows such private release while providing strong guarantees on the output. A basic mechanism achieves differential privacy by adding noise to the frequency counts in the contingency tables (or, a subset of the count data cube) derived from the dataset. However, when the dataset is sparse in its underlying space, as is the case for most multi-attribute relations, then the effect of adding noise is to vastly increase the size of the published data: it implicitly creates a huge number of dummy data points to mask the true data, making it almost impossible to work with.},
	language = {en},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Cormode, Graham and Procopiuc, Magda and Srivastava, Divesh and Tran, Thanh T. L.},
	month = mar,
	year = {2011},
	note = {arXiv:1103.0825 [cs]},
	keywords = {Computer Science - Databases},
}

@misc{gupta_differentially_2009-1,
	title = {Differentially {Private} {Combinatorial} {Optimization}},
	url = {http://arxiv.org/abs/0903.4510},
	doi = {10.48550/arXiv.0903.4510},
	abstract = {Consider the following problem: given a metric space, some of whose points are "clients", open a set of at most \$k\$ facilities to minimize the average distance from the clients to these facilities. This is just the well-studied \$k\$-median problem, for which many approximation algorithms and hardness results are known. Note that the objective function encourages opening facilities in areas where there are many clients, and given a solution, it is often possible to get a good idea of where the clients are located. However, this poses the following quandary: what if the identity of the clients is sensitive information that we would like to keep private? Is it even possible to design good algorithms for this problem that preserve the privacy of the clients? In this paper, we initiate a systematic study of algorithms for discrete optimization problems in the framework of differential privacy (which formalizes the idea of protecting the privacy of individual input elements). We show that many such problems indeed have good approximation algorithms that preserve differential privacy; this is even in cases where it is impossible to preserve cryptographic definitions of privacy while computing any non-trivial approximation to even the\_value\_ of an optimal solution, let alone the entire solution. Apart from the \$k\$-median problem, we study the problems of vertex and set cover, min-cut, facility location, Steiner tree, and the recently introduced submodular maximization problem, "Combinatorial Public Projects" (CPP).},
	urldate = {2024-08-01},
	publisher = {arXiv},
	author = {Gupta, Anupam and Ligett, Katrina and McSherry, Frank and Roth, Aaron and Talwar, Kunal},
	month = nov,
	year = {2009},
	note = {arXiv:0903.4510 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms},
}

@article{pagh_faster_nodate,
	title = {Faster {Private} {Minimum} {Spanning} {Trees}},
	language = {en},
	author = {Pagh, Rasmus and Retschmeier, Lukas},
}

@book{baier_principles_2008,
	address = {Cambridge, Mass.},
	title = {Principles of model checking},
	isbn = {978-0-262-02649-9},
	language = {en},
	publisher = {MIT Press},
	author = {Baier, Christel and Katoen, Joost-Pieter},
	year = {2008},
}

@book{diestel_graph_2000,
	address = {New York},
	edition = {2nd ed},
	series = {Graduate texts in mathematics},
	title = {Graph theory},
	isbn = {978-0-387-95014-3 978-0-387-98976-1},
	language = {en},
	number = {173},
	publisher = {Springer},
	author = {Diestel, Reinhard},
	year = {2000},
	keywords = {Graph theory},
}

@book{motwani_randomized_2007,
	address = {Cambridge New York Melbourne},
	edition = {9th printing},
	title = {Randomized algorithms},
	isbn = {978-0-521-47465-8},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Motwani, Rajeev and Raghavan, Prabhakar},
	year = {2007},
}

@article{kamath_lecture_nodate,
	title = {Lecture 3 — {Intro} to {Diﬀerential} {Privacy}},
	language = {en},
	author = {Kamath, Gautam and Kamath, Gautam},
}

@article{rotenberg_weekplan_nodate,
	title = {Weekplan: {Approximate} {Distance} {Oracles}},
	language = {en},
	author = {Rotenberg, Eva},
}

@book{tanenbaum_operating_2006,
	address = {Upper Saddle River, N.J},
	edition = {3rd ed},
	title = {Operating systems: design and implementation},
	isbn = {978-0-13-142938-3 978-0-13-142987-1},
	shorttitle = {Operating systems},
	language = {en},
	publisher = {Pearson/Prentice Hall},
	author = {Tanenbaum, Andrew S. and Woodhull, Albert S.},
	year = {2006},
	note = {OCLC: ocm61859929},
	keywords = {MINIX, Operating systems (Computers)},
}

@article{giraud_computational_2015,
	title = {Computational {Fugue} {Analysis}},
	volume = {39},
	url = {https://hal.science/hal-01113520},
	doi = {10.1162/COMJ_a_00300},
	abstract = {One of the pinnacle forms of classical Western music, the fugue is often used in the teaching of music analysis and composition. Fugues alternate between instances of a subject and other patterns and modulatory sections, which are called episodes. Musicological analyses are generally built on these patterns and sections.
We propose several algorithms to perform an automated analysis of a fugue, starting from a score in which all the voices are separated. By focusing on the diatonic similarities between pitch intervals, we detect subjects and countersubjects, as well as partial harmonic sequences inside the episodes. We also propose tools to detect subject scale degrees, cadences and pedals, as well as a method for segmenting the fugue into exposition and episodic parts.
Our algorithms were tested on a corpus of 36 fugues by Bach and Shostakovich. We provide formalized ground truth data on this corpus as well as a dynamic visualization of the ground truth and of our computed results. The complete system showed correct or good results for about one half of the fugues tested, enabling us to sketch their design.},
	number = {2},
	urldate = {2024-07-15},
	journal = {Computer Music Journal},
	author = {Giraud, Mathieu and Groult, Richard and Leguy, Emmanuel and Levé, Florence},
	year = {2015},
	note = {Publisher: Massachusetts Institute of Technology Press (MIT Press)},
	keywords = {computational music analysis, contrapuntal music, fugues, musical structure, repeating patterns},
	pages = {77--96},
}

@article{thorup_approximate_2005,
	title = {Approximate distance oracles},
	volume = {52},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/1044731.1044732},
	doi = {10.1145/1044731.1044732},
	abstract = {Let G = (V,E) be an undirected weighted graph with {\textbar}V{\textbar} = n and {\textbar}E{\textbar} = m. Let k ≥ 1 be an integer. We show that G = (V,E) can be preprocessed in O(kmn1/k) expected time, constructing a data structure of size O(kn1+1/k), such that any subsequent distance query can be answered, approximately, in O(k) time. The approximate distance returned is of stretch at most 2k−1, that is, the quotient obtained by dividing the estimated distance by the actual distance lies between 1 and 2k−1. A 1963 girth conjecture of Erdós, implies that Ω(n1+1/k) space is needed in the worst case for any real stretch strictly smaller than 2k+1. The space requirement of our algorithm is, therefore, essentially optimal. The most impressive feature of our data structure is its constant query time, hence the name "oracle". Previously, data structures that used only O(n1+1/k) space had a query time of Ω(n1/k).Our algorithms are extremely simple and easy to implement efficiently. They also provide faster constructions of sparse spanners of weighted graphs, and improved tree covers and distance labelings of weighted or unweighted graphs.},
	number = {1},
	urldate = {2024-07-12},
	journal = {J. ACM},
	author = {Thorup, Mikkel and Zwick, Uri},
	month = jan,
	year = {2005},
	pages = {1--24},
}

@inproceedings{bafna_price_2017,
	title = {The {Price} of {Selection} in {Differential} {Privacy}},
	url = {https://proceedings.mlr.press/v65/bafna17a.html},
	abstract = {In the differentially private top-\$k\$ selection problem, we are given a dataset \$X ∈{\textbackslash}pmo{\textasciicircum}n {\textbackslash}times d\$, in which each row belongs to an individual and each column corresponds to some binary attribute, and our goal is to find a set of \$k ≪d\$ columns whose means are approximately as large as possible.  Differential privacy requires that our choice of these \$k\$ columns does not depend too much on any on individual’s dataset.  This problem can be solved using the well known exponential mechanism and composition properties of differential privacy.  In the high-accuracy regime, where we require the error of the selection procedure to be to be smaller than the so-called sampling error \$α≈{\textbackslash}sqrt{\textbackslash}ln(d)/n\$, this procedure succeeds given a dataset of size \$n ≳k {\textbackslash}ln(d)\$. We prove a matching lower bound, showing that a dataset of size \$n ≳k {\textbackslash}ln(d)\$ is necessary for private top-\$k\$ selection in this high-accuracy regime.  Our lower bound shows that selecting the \$k\$ largest columns requires more data than simply estimating the value of those \$k\$ columns, which can be done using a dataset of size just \$n ≳k\$.},
	language = {en},
	urldate = {2024-07-05},
	booktitle = {Proceedings of the 2017 {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Bafna, Mitali and Ullman, Jonathan},
	month = jun,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {151--168},
}

@article{erdos_random_1959,
	title = {On {Random} {Graphs} {I}},
	volume = {6},
	journal = {Publicationes Mathematicae Debrecen},
	author = {Erdös, P. and Rényi, A.},
	year = {1959},
	keywords = {graph sna},
	pages = {290},
}

@article{noauthor_random_nodate,
	title = {On {Random} {Graphs} {I}},
}

@misc{noauthor_random_nodate-1,
	title = {On {Random} {Graphs} {I} {\textbar} {BibSonomy}},
	url = {https://www.bibsonomy.org/bibtex/25aab47a7be9ec47644735f8e0a4607b6/alex},
	urldate = {2024-07-04},
}

@misc{noauthor_random_nodate-2,
	title = {On {Random} {Graphs} {I} {\textbar} {BibSonomy}},
	url = {https://www.bibsonomy.org/bibtex/25aab47a7be9ec47644735f8e0a4607b6/alex},
	urldate = {2024-07-04},
}

@misc{steinke_tight_2017,
	title = {Tight {Lower} {Bounds} for {Differentially} {Private} {Selection}},
	url = {http://arxiv.org/abs/1704.03024},
	doi = {10.48550/arXiv.1704.03024},
	abstract = {A pervasive task in the differential privacy literature is to select the \$k\$ items of "highest quality" out of a set of \$d\$ items, where the quality of each item depends on a sensitive dataset that must be protected. Variants of this task arise naturally in fundamental problems like feature selection and hypothesis testing, and also as subroutines for many sophisticated differentially private algorithms. The standard approaches to these tasks---repeated use of the exponential mechanism or the sparse vector technique---approximately solve this problem given a dataset of \$n = O({\textbackslash}sqrt\{k\}{\textbackslash}log d)\$ samples. We provide a tight lower bound for some very simple variants of the private selection problem. Our lower bound shows that a sample of size \$n = {\textbackslash}Omega({\textbackslash}sqrt\{k\} {\textbackslash}log d)\$ is required even to achieve a very minimal accuracy guarantee. Our results are based on an extension of the fingerprinting method to sparse selection problems. Previously, the fingerprinting method has been used to provide tight lower bounds for answering an entire set of \$d\$ queries, but often only some much smaller set of \$k\$ queries are relevant. Our extension allows us to prove lower bounds that depend on both the number of relevant queries and the total number of queries.},
	urldate = {2024-07-04},
	publisher = {arXiv},
	author = {Steinke, Thomas and Ullman, Jonathan},
	month = apr,
	year = {2017},
	note = {arXiv:1704.03024 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms},
}

@book{retschmeier_masters_nodate,
	title = {Masters {Work}},
	author = {Retschmeier, Lukas},
}

@article{retschmeier_masters_nodate-1,
	title = {Master's {Thesis} {Presentation} - {On} the {Parameterized} {Complexity} of   {Semitotal} {Dominating} {Set} on {Graph} {Classes}},
	language = {en},
	author = {Retschmeier, Lukas},
}

@article{retschmeier_masters_nodate-2,
	title = {Master’s {Thesis} in {Informatics}},
	language = {en},
	author = {Retschmeier, Lukas},
}

@article{retschmeier_entwicklung_nodate,
	title = {Entwicklung und {Evaluation} eines {Information} {Retrieval} {Systems} für die {Unterstützung} des {Betriebs} von {PLM} {Systemen}},
	language = {de},
	author = {Retschmeier, Lukas},
}

@book{retschmeier_bachelor_nodate,
	title = {Bachelor {Work}},
	author = {Retschmeier, Lukas},
}

@article{retschmeier_parametrized_nodate,
	title = {Parametrized {Complexity} - {Seminar}: {Advanced} {Algorithms}},
	language = {en},
	author = {Retschmeier, Lukas},
}

@article{retschmeier_faster_2024,
	series = {{TPDP}},
	title = {Faster {Private} {Minimum} {Spanning} {Trees}},
	abstract = {We consider the problem of releasing a minimum spanning tree (MST) under edge-weight differential privacy constraints where a graph topology G = (V, E) with n vertices is public and the weight matrix W ∈ Rn×n is kept private using ρ-zCDP. In this work, we consider the ℓ∞ neighboring relationship on the weight matrix and denote the sensitivity as ∆∞.},
	language = {en},
	author = {Retschmeier, Lukas and Pagh, Rasmus},
	year = {2024},
}

@article{retschmeier_masters_nodate-3,
	title = {Master’s {Thesis} in {Informatics}},
	language = {en},
	author = {Retschmeier, Lukas},
}

@misc{hu_sok_2023,
	title = {{SoK}: {Privacy}-{Preserving} {Data} {Synthesis}},
	shorttitle = {{SoK}},
	url = {http://arxiv.org/abs/2307.02106},
	doi = {10.48550/arXiv.2307.02106},
	abstract = {As the prevalence of data analysis grows, safeguarding data privacy has become a paramount concern. Consequently, there has been an upsurge in the development of mechanisms aimed at privacy-preserving data analyses. However, these approaches are task-specific; designing algorithms for new tasks is a cumbersome process. As an alternative, one can create synthetic data that is (ideally) devoid of private information. This paper focuses on privacy-preserving data synthesis (PPDS) by providing a comprehensive overview, analysis, and discussion of the field. Specifically, we put forth a master recipe that unifies two prominent strands of research in PPDS: statistical methods and deep learning (DL)-based methods. Under the master recipe, we further dissect the statistical methods into choices of modeling and representation, and investigate the DL-based methods by different generative modeling principles. To consolidate our findings, we provide comprehensive reference tables, distill key takeaways, and identify open problems in the existing literature. In doing so, we aim to answer the following questions: What are the design principles behind different PPDS methods? How can we categorize these methods, and what are the advantages and disadvantages associated with each category? Can we provide guidelines for method selection in different real-world scenarios? We proceed to benchmark several prominent DL-based methods on the task of private image synthesis and conclude that DP-MERF is an all-purpose approach. Finally, upon systematizing the work over the past decade, we identify future directions and call for actions from researchers.},
	urldate = {2024-07-03},
	publisher = {arXiv},
	author = {Hu, Yuzheng and Wu, Fan and Li, Qinbin and Long, Yunhui and Garrido, Gonzalo Munilla and Ge, Chang and Ding, Bolin and Forsyth, David and Li, Bo and Song, Dawn},
	month = aug,
	year = {2023},
	note = {arXiv:2307.02106 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Databases, Computer Science - Machine Learning},
}

@misc{hardt_simple_2012,
	title = {A simple and practical algorithm for differentially private data release},
	url = {http://arxiv.org/abs/1012.4763},
	doi = {10.48550/arXiv.1012.4763},
	abstract = {We present new theoretical results on differentially private data release useful with respect to any target class of counting queries, coupled with experimental results on a variety of real world data sets. Specifically, we study a simple combination of the multiplicative weights approach of [Hardt and Rothblum, 2010] with the exponential mechanism of [McSherry and Talwar, 2007]. The multiplicative weights framework allows us to maintain and improve a distribution approximating a given data set with respect to a set of counting queries. We use the exponential mechanism to select those queries most incorrectly tracked by the current distribution. Combing the two, we quickly approach a distribution that agrees with the data set on the given set of queries up to small error. The resulting algorithm and its analysis is simple, but nevertheless improves upon previous work in terms of both error and running time. We also empirically demonstrate the practicality of our approach on several data sets commonly used in the statistical community for contingency table release.},
	urldate = {2024-07-03},
	publisher = {arXiv},
	author = {Hardt, Moritz and Ligett, Katrina and McSherry, Frank},
	month = mar,
	year = {2012},
	note = {arXiv:1012.4763 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@article{zhang_privbayes_nodate,
	title = {{PrivBayes}: {Private} {Data} {Release} via {Bayesian} {Networks}},
	abstract = {Privacy-preserving data publishing is an important problem that has been the focus of extensive study. The state-of-the-art goal for this problem is differential privacy, which offers a strong degree of privacy protection without making restrictive assumptions about the adversary. Existing techniques using differential privacy, however, cannot effectively handle the publication of high-dimensional data. In particular, when the input dataset contains a large number of attributes, existing methods require injecting a prohibitive amount of noise compared to the signal in the data, which renders the published data next to useless.},
	language = {en},
	author = {Zhang, Jun and Cormode, Graham and Procopiuc, Cecilia M and Srivastava, Divesh and Xiao, Xiaokui},
}

@inproceedings{jordon_pate-gan_2018,
	title = {{PATE}-{GAN}: {Generating} {Synthetic} {Data} with {Differential} {Privacy} {Guarantees}},
	shorttitle = {{PATE}-{GAN}},
	url = {https://openreview.net/forum?id=S1zk9iRqF7},
	abstract = {Machine learning has the potential to assist many communities in using the large datasets that are becoming more and more available. Unfortunately, much of that potential is not being realized because it would require sharing data in a way that compromises privacy. In this paper, we investigate a method for ensuring (differential) privacy of the generator of the Generative Adversarial Nets (GAN) framework. The resulting model can be used for generating synthetic data on which algorithms can be trained and validated, and on which competitions can be conducted, without compromising the privacy of the original dataset. Our method modifies the Private Aggregation of Teacher Ensembles (PATE) framework and applies it to GANs. Our modified framework (which we call PATE-GAN) allows us to tightly bound the influence of any individual sample on the model, resulting in tight differential privacy guarantees and thus an improved performance over models with the same guarantees. We also look at measuring the quality of synthetic data from a new angle; we assert that for the synthetic data to be useful for machine learning researchers, the relative performance of two algorithms (trained and tested) on the synthetic dataset should be the same as their relative performance (when trained and tested) on the original dataset. Our experiments, on various datasets, demonstrate that PATE-GAN consistently outperforms the state-of-the-art method with respect to this and other notions of synthetic data quality.},
	language = {en},
	urldate = {2024-07-03},
	author = {Jordon, James and Yoon, Jinsung and Schaar, Mihaela van der},
	month = sep,
	year = {2018},
}

@misc{mckenna_winning_2021,
	title = {Winning the {NIST} {Contest}: {A} scalable and general approach to differentially private synthetic data},
	shorttitle = {Winning the {NIST} {Contest}},
	url = {http://arxiv.org/abs/2108.04978},
	abstract = {We propose a general approach for diﬀerentially private synthetic data generation, that consists of three steps: (1) select a collection of low-dimensional marginals, (2) measure those marginals with a noise addition mechanism, and (3) generate synthetic data that preserves the measured marginals well. Central to this approach is Private-PGM [42], a post-processing method that is used to estimate a high-dimensional data distribution from noisy measurements of its marginals. We present two mechanisms, NIST-MST and MST, that are instances of this general approach. NIST-MST was the winning mechanism in the 2018 NIST diﬀerential privacy synthetic data competition, and MST is a new mechanism that can work in more general settings, while still performing comparably to NIST-MST. We believe our general approach should be of broad interest, and can be adopted in future mechanisms for synthetic data generation.},
	language = {en},
	urldate = {2024-07-03},
	publisher = {arXiv},
	author = {McKenna, Ryan and Miklau, Gerome and Sheldon, Daniel},
	month = aug,
	year = {2021},
	note = {arXiv:2108.04978 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{mckenna_graphical-model_2019,
	title = {Graphical-model based estimation and inference for differential privacy},
	url = {http://arxiv.org/abs/1901.09136},
	doi = {10.48550/arXiv.1901.09136},
	abstract = {Many privacy mechanisms reveal high-level information about a data distribution through noisy measurements. It is common to use this information to estimate the answers to new queries. In this work, we provide an approach to solve this estimation problem efficiently using graphical models, which is particularly effective when the distribution is high-dimensional but the measurements are over low-dimensional marginals. We show that our approach is far more efficient than existing estimation techniques from the privacy literature and that it can improve the accuracy and scalability of many state-of-the-art mechanisms.},
	urldate = {2024-07-03},
	publisher = {arXiv},
	author = {McKenna, Ryan and Sheldon, Daniel and Miklau, Gerome},
	month = jan,
	year = {2019},
	note = {arXiv:1901.09136 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cai_data_2021,
	title = {Data synthesis via differentially private markov random fields},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3476249.3476272},
	doi = {10.14778/3476249.3476272},
	abstract = {This paper studies the synthesis of high-dimensional datasets with differential privacy (DP). The state-of-the-art solution addresses this problem by first generating a set M of noisy low-dimensional marginals of the input data 𝐷, and then use them to approximate the data distribution in 𝐷 for synthetic data generation. However, it imposes several constraints on M that considerably limits the choices of marginals. This makes it difficult to capture all important correlations among attributes, which in turn degrades the quality of the resulting synthetic data.},
	language = {en},
	number = {11},
	urldate = {2024-07-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Cai, Kuntai and Lei, Xiaoyu and Wei, Jianxin and Xiao, Xiaokui},
	month = jul,
	year = {2021},
	pages = {2190--2202},
}

@incollection{hutchison_pcps_2011,
	address = {Berlin, Heidelberg},
	title = {{PCPs} and the {Hardness} of {Generating} {Private} {Synthetic} {Data}},
	volume = {6597},
	isbn = {978-3-642-19570-9 978-3-642-19571-6},
	url = {http://link.springer.com/10.1007/978-3-642-19571-6_24},
	abstract = {Assuming the existence of one-way functions, we show that there is no polynomial-time, differentially private algorithm A that takes a database D ∈ (\{0, 1\}d)n and outputs a “synthetic database” ̂D all of whose two-way marginals are approximately equal to those of D. (A two-way marginal is the fraction of database rows x ∈ \{0, 1\}d with a given pair of values in a given pair of columns.) This answers a question of Barak et al. (PODS ‘07), who gave an algorithm running in time poly(n, 2d).},
	language = {en},
	urldate = {2024-07-03},
	booktitle = {Theory of {Cryptography}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ullman, Jonathan and Vadhan, Salil},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Ishai, Yuval},
	year = {2011},
	doi = {10.1007/978-3-642-19571-6_24},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {400--416},
}

@misc{feldman_hiding_2021,
	title = {Hiding {Among} the {Clones}: {A} {Simple} and {Nearly} {Optimal} {Analysis} of {Privacy} {Amplification} by {Shuffling}},
	shorttitle = {Hiding {Among} the {Clones}},
	url = {http://arxiv.org/abs/2012.12803},
	doi = {10.48550/arXiv.2012.12803},
	abstract = {Recent work of Erlingsson, Feldman, Mironov, Raghunathan, Talwar, and Thakurta [EFMRTT19] demonstrates that random shuffling amplifies differential privacy guarantees of locally randomized data. Such amplification implies substantially stronger privacy guarantees for systems in which data is contributed anonymously [BEMMRLRKTS17] and has lead to significant interest in the shuffle model of privacy [CSUZZ19; EFMRTT19]. We show that random shuffling of \$n\$ data records that are input to \${\textbackslash}varepsilon\_0\$-differentially private local randomizers results in an \$(O((1-e{\textasciicircum}\{-{\textbackslash}varepsilon\_0\}){\textbackslash}sqrt\{{\textbackslash}frac\{e{\textasciicircum}\{{\textbackslash}varepsilon\_0\}{\textbackslash}log(1/{\textbackslash}delta)\}\{n\}\}), {\textbackslash}delta)\$-differentially private algorithm. This significantly improves over previous work and achieves the asymptotically optimal dependence in \${\textbackslash}varepsilon\_0\$. Our result is based on a new approach that is simpler than previous work and extends to approximate differential privacy with nearly the same guarantees. Importantly, our work also yields an algorithm for deriving tighter bounds on the resulting \${\textbackslash}varepsilon\$ and \${\textbackslash}delta\$ as well as R{\textbackslash}'enyi differential privacy guarantees. We show numerically that our algorithm gets to within a small constant factor of the optimal bound. As a direct corollary of our analysis we derive a simple and nearly optimal algorithm for frequency estimation in the shuffle model of privacy. We also observe that our result implies the first asymptotically optimal privacy analysis of noisy stochastic gradient descent that applies to sampling without replacement.},
	urldate = {2024-07-03},
	publisher = {arXiv},
	author = {Feldman, Vitaly and McMillan, Audra and Talwar, Kunal},
	month = sep,
	year = {2021},
	note = {arXiv:2012.12803 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{cheu_differential_2022,
	title = {Differential {Privacy} in the {Shuffle} {Model}: {A} {Survey} of {Separations}},
	shorttitle = {Differential {Privacy} in the {Shuffle} {Model}},
	url = {http://arxiv.org/abs/2107.11839},
	abstract = {Differential privacy is often studied in one of two models. In the central model, a single analyzer has the responsibility of performing a privacy-preserving computation on data. But in the local model, each data owner ensures their own privacy. Although it removes the need to trust the analyzer, local privacy comes at a price: a locally private protocol is less accurate than a centrally private counterpart when solving many learning and estimation problems. Protocols in the shuffle model are designed to attain the best of both worlds: recent work has shown high accuracy is possible with only a mild trust assumption. This survey paper gives an overview of novel shuffle protocols, along with lower bounds that establish the limits of the new model. We also summarize work that show the promise of interactivity in the shuffle model.},
	language = {en},
	urldate = {2024-07-03},
	publisher = {arXiv},
	author = {Cheu, Albert},
	month = may,
	year = {2022},
	note = {arXiv:2107.11839 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms},
}

@incollection{cheu_distributed_2019,
	title = {Distributed {Differential} {Privacy} via {Shuffling}},
	volume = {11476},
	url = {http://arxiv.org/abs/1808.01394},
	abstract = {We consider the problem of designing scalable, robust protocols for computing statistics about sensitive data. Speciﬁcally, we look at how best to design differentially private protocols in a distributed setting, where each user holds a private datum. The literature has mostly considered two models: the “central” model, in which a trusted server collects users’ data in the clear, which allows greater accuracy; and the “local” model, in which users individually randomize their data, and need not trust the server, but accuracy is limited. Attempts to achieve the accuracy of the central model without a trusted server have so far focused on variants of cryptographic secure function evaluation, which limits scalability.},
	language = {en},
	urldate = {2024-07-03},
	author = {Cheu, Albert and Smith, Adam and Ullman, Jonathan and Zeber, David and Zhilyaev, Maxim},
	year = {2019},
	doi = {10.1007/978-3-030-17653-2_13},
	note = {arXiv:1808.01394 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Data Structures and Algorithms},
	pages = {375--403},
}

@inproceedings{cormode_differentially_2012,
	address = {Berlin Germany},
	title = {Differentially private summaries for sparse data},
	isbn = {978-1-4503-0791-8},
	url = {https://dl.acm.org/doi/10.1145/2274576.2274608},
	doi = {10.1145/2274576.2274608},
	abstract = {Differential privacy is fast becoming the method of choice for releasing data under strong privacy guarantees. A standard mechanism is to add noise to the counts in contingency tables derived from the dataset. However, when the dataset is sparse in its underlying domain, this vastly increases the size of the published data, to the point of making the mechanism infeasible.},
	language = {en},
	urldate = {2024-07-03},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Database} {Theory}},
	publisher = {ACM},
	author = {Cormode, Graham and Procopiuc, Cecilia and Srivastava, Divesh and Tran, Thanh T. L.},
	month = mar,
	year = {2012},
	pages = {299--311},
}

@inproceedings{mcgregor_limits_2010,
	address = {Las Vegas, NV, USA},
	title = {The {Limits} of {Two}-{Party} {Differential} {Privacy}},
	isbn = {978-1-4244-8525-3},
	url = {http://ieeexplore.ieee.org/document/5670946/},
	doi = {10.1109/FOCS.2010.14},
	abstract = {We study diﬀerential privacy in a distributed setting where two parties would like to perform analysis of their joint data while preserving privacy for both datasets. Our results imply almost tight lower bounds on the accuracy of such data analyses, both for speciﬁc natural functions (such as Hamming distance) and in general. Our bounds expose a sharp contrast between the two-party setting and the simpler client-server setting (where privacy guarantees are one-sided). In addition, those bounds demonstrate a dramatic gap between the accuracy that can be obtained by diﬀerentially private data analysis versus the accuracy obtainable when privacy is relaxed to a computational variant of diﬀerential privacy.},
	language = {en},
	urldate = {2024-07-03},
	booktitle = {2010 {IEEE} 51st {Annual} {Symposium} on {Foundations} of {Computer} {Science}},
	publisher = {IEEE},
	author = {McGregor, Andrew and Mironov, Ilya and Pitassi, Toniann and Reingold, Omer and Talwar, Kunal and Vadhan, Salil},
	month = oct,
	year = {2010},
	pages = {81--90},
}

@inproceedings{bateni_affinity_2017,
	title = {Affinity {Clustering}: {Hierarchical} {Clustering} at {Scale}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/2e1b24a664f5e9c18f407b2f9c73e821-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2017, {December} 4-9, 2017, {Long} {Beach}, {CA}, {USA}},
	author = {Bateni, MohammadHossein and Behnezhad, Soheil and Derakhshan, Mahsa and Hajiaghayi, MohammadTaghi and Kiveris, Raimondas and Lattanzi, Silvio and Mirrokni, Vahab S.},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	pages = {6864--6874},
}

@article{lai_approximate_2009,
	title = {Approximate minimum spanning tree clustering in high-dimensional space},
	volume = {13},
	url = {https://doi.org/10.3233/IDA-2009-0382},
	doi = {10.3233/IDA-2009-0382},
	number = {4},
	journal = {Intell. Data Anal.},
	author = {Lai, Chih and Rafa, Taras and Nelson, Dwight E.},
	year = {2009},
	pages = {575--597},
}

@inproceedings{mitrovic_differentially_2017,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Differentially {Private} {Submodular} {Maximization}: {Data} {Summarization} in {Disguise}},
	volume = {70},
	url = {https://proceedings.mlr.press/v70/mitrovic17a.html},
	abstract = {Many data summarization applications are captured by the general framework of submodular maximization. As a consequence, a wide range of efficient approximation algorithms have been developed. However, when such applications involve sensitive data about individuals, their privacy concerns are not automatically addressed. To remedy this problem, we propose a general and systematic study of differentially private submodular maximization. We present privacy-preserving algorithms for both monotone and non-monotone submodular maximization under cardinality, matroid, and p-extendible system constraints, with guarantees that are competitive with optimal. Along the way, we analyze a new algorithm for non-monotone submodular maximization, which is the first (even non-privately) to achieve a constant approximation ratio while running in linear time. We additionally provide two concrete experiments to validate the efficacy of these algorithms.},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mitrovic, Marko and Bun, Mark and Krause, Andreas and Karbasi, Amin},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = aug,
	year = {2017},
	pages = {2478--2487},
}

@inproceedings{hay_accurate_2009,
	title = {Accurate {Estimation} of the {Degree} {Distribution} of {Private} {Networks}},
	doi = {10.1109/ICDM.2009.11},
	booktitle = {2009 {Ninth} {IEEE} {International} {Conference} on {Data} {Mining}},
	author = {Hay, Michael and Li, Chao and Miklau, Gerome and Jensen, David},
	year = {2009},
	keywords = {Algorithm design and analysis, Analysis of variance, Chaotic communication, Communication networks, Computer science, Data mining, Data privacy, Diseases, Distortion measurement, Social network services, differential privacy, privacy, privacy-preserving data mining, social networks},
	pages = {169--178},
}

@inproceedings{costea_qualitative_2013,
	title = {Qualitative analysis of differential privacy applied over graph structures},
	doi = {10.1109/RoEduNet.2013.6511749},
	booktitle = {2013 11th {RoEduNet} {International} {Conference}},
	author = {Costea, Sergiu and Barbu, Marian and Rughinis, Razvan},
	year = {2013},
	pages = {1--4},
}

@article{hladik_near-universally-optimal_2024,
	title = {Near-{Universally}-{Optimal} {Differentially} {Private} {Minimum} {Spanning} {Trees}},
	journal = {arXiv e-prints},
	author = {Hladík, Richard and Tětek, Jakub},
	year = {2024},
	note = {Publisher: arXiv Preprint
\_eprint: 2404.15035},
}

@article{chow_approximating_1968,
	title = {Approximating discrete probability distributions with dependence trees},
	volume = {14},
	doi = {10.1109/TIT.1968.1054142},
	number = {3},
	journal = {IEEE Transactions on Information Theory},
	author = {Chow, C. and Liu, C.},
	year = {1968},
	pages = {462--467},
}

@inproceedings{bun_concentrated_2016,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Concentrated {Differential} {Privacy}: {Simplifications}, {Extensions}, and {Lower} {Bounds}},
	isbn = {978-3-662-53641-4},
	shorttitle = {Concentrated {Differential} {Privacy}},
	doi = {10.1007/978-3-662-53641-4_24},
	booktitle = {Theory of {Cryptography}},
	publisher = {Springer},
	author = {Bun, Mark and Steinke, Thomas},
	editor = {Hirt, Martin and Smith, Adam},
	year = {2016},
	pages = {635--658},
}

@inproceedings{pinot_graph-based_2018,
	address = {Monterey, California, United States},
	title = {Graph-based {Clustering} under {Differential} {Privacy}},
	url = {https://hal.science/hal-02170699},
	booktitle = {Conference on {Uncertainty} in {Artificial} {Intelligence} ({UAI} 2018)},
	author = {Pinot, Rafael and Morvan, Anne and Yger, Florian and Gouy-Pailler, Cedric and Atif, Jamal},
	month = aug,
	year = {2018},
	keywords = {Artificial inteligence, Minimum Spanning Tree, arbitrary-shaped node cluster, clustering, graph, nonconvex clustering partition, statistical analysis, weight differential privacy constraint},
	pages = {329--338},
}

@article{pinot_minimum_2018,
	title = {Minimum spanning tree release under differential privacy constraints},
	journal = {arXiv e-prints (Master Thesis)},
	author = {Pinot, Rafael},
	year = {2018},
	note = {\_eprint: 1801.06423},
}

@article{jarnik_o_1930,
	title = {O jistém problému minimálním. ({Z} dopisu panu {O}. {Borůvkovi})},
	volume = {6},
	language = {Czech},
	journal = {Práce moravské přirodovědecké společnosti},
	author = {Jarník, Vojtěch},
	year = {1930},
	pages = {57--63},
}

@article{kruskal_shortest_1956,
	title = {On the {Shortest} {Spanning} {Subtree} of a {Graph} and the {Traveling} {Salesman} {Problem}},
	volume = {7},
	issn = {00029939, 10886826},
	url = {http://www.jstor.org/stable/2033241},
	number = {1},
	urldate = {2024-04-15},
	journal = {Proceedings of the American Mathematical Society},
	author = {Kruskal, Joseph B.},
	year = {1956},
	note = {Publisher: American Mathematical Society},
	pages = {48--50},
}

@inproceedings{mckenna_permute-and-flip_2020,
	title = {Permute-and-{Flip}: {A} new mechanism for differentially private selection},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/01e00f2f4bfcbb7505cb641066f2859b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {McKenna, Ryan and Sheldon, Daniel R},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {193--203},
}

@article{prim_shortest_1957,
	title = {Shortest connection networks and some generalizations},
	volume = {36},
	issn = {0005-8580},
	doi = {10.1002/j.1538-7305.1957.tb01515.x},
	abstract = {The basic problem considered is that of interconnecting a given set of terminals with a shortest possible network of direct links. Simple and practical procedures are given for solving this problem both graphically and computationally. It develops that these procedures also provide solutions for a much broader class of problems, containing other examples of practical interest.},
	number = {6},
	journal = {The Bell System Technical Journal},
	author = {Prim, Robert Clay},
	month = nov,
	year = {1957},
	note = {Publisher: Nokia Bell Labs},
	pages = {1389--1401},
}

@misc{noauthor_sqrt_nodate,
	title = {Sqrt {Decomposition} - {Algorithms} for {Competitive} {Programming} — cp-algorithms.com},
	url = {https://cp-algorithms.com/data_structures/sqrt_decomposition.html},
}

@article{ding_permute-and-flip_2021,
	title = {The {Permute}-and-{Flip} {Mechanism} is {Identical} to {Report}-{Noisy}-{Max} with {Exponential} {Noise}},
	doi = {10.48550/arXiv.2105.07260},
	journal = {arXiv e-prints},
	author = {Ding, Zeyu and Kifer, Daniel and Sayed M. Saghaian N., E. and Steinke, Thomas and Wang, Yuxin and Xiao, Yingtai and Zhang, Danfeng},
	month = may,
	year = {2021},
	note = {\_eprint: 2105.07260},
	keywords = {Computer Science - Cryptography and Security},
	pages = {arXiv:2105.07260},
}

@inproceedings{jayaram_massively_2024,
	title = {Massively {Parallel} {Algorithms} for {High}-{Dimensional} {Euclidean} {Minimum} {Spanning} {Tree}},
	url = {https://doi.org/10.1137/1.9781611977912.139},
	doi = {10.1137/1.9781611977912.139},
	booktitle = {Proceedings of the 2024 {ACM}-{SIAM} {Symposium} on {Discrete} {Algorithms}, {SODA} 2024, {Alexandria}, {VA}, {USA}, {January} 7-10, 2024},
	publisher = {SIAM},
	author = {Jayaram, Rajesh and Mirrokni, Vahab and Narayanan, Shyam and Zhong, Peilin},
	editor = {Woodruff, David P.},
	year = {2024},
	pages = {3960--3996},
}

@inproceedings{kasiviswanathan_analyzing_2013,
	address = {Berlin, Heidelberg},
	series = {{TCC}'13},
	title = {Analyzing graphs with node differential privacy},
	isbn = {978-3-642-36593-5},
	url = {https://doi.org/10.1007/978-3-642-36594-2_26},
	doi = {10.1007/978-3-642-36594-2_26},
	abstract = {We develop algorithms for the private analysis of network data that provide accurate analysis of realistic networks while satisfying stronger privacy guarantees than those of previous work. We present several techniques for designing node differentially private algorithms, that is, algorithms whose output distribution does not change significantly when a node and all its adjacent edges are added to a graph. We also develop methodology for analyzing the accuracy of such algorithms on realistic networks.The main idea behind our techniques is to 'project' (in one of several senses) the input graph onto the set of graphs with maximum degree below a certain threshold. We design projection operators, tailored to specific statistics that have low sensitivity and preserve information about the original statistic. These operators can be viewed as giving a fractional (low-degree) graph that is a solution to an optimization problem described as a maximum flow instance, linear program, or convex program. In addition, we derive a generic, efficient reduction that allows us to apply any differentially private algorithm for bounded-degree graphs to an arbitrary graph. This reduction is based on analyzing the smooth sensitivity of the 'naive' truncation that simply discards nodes of high degree.},
	booktitle = {Proceedings of the 10th {Theory} of {Cryptography} {Conference} on {Theory} of {Cryptography}},
	publisher = {Springer-Verlag},
	author = {Kasiviswanathan, Shiva Prasad and Nissim, Kobbi and Raskhodnikova, Sofya and Smith, Adam},
	year = {2013},
	note = {event-place: Tokyo, Japan},
	pages = {457--476},
}

@inproceedings{dwork_calibrating_2006,
	address = {Berlin, Heidelberg},
	title = {Calibrating {Noise} to {Sensitivity} in {Private} {Data} {Analysis}},
	isbn = {978-3-540-32732-5},
	doi = {10.1007/11681878_14},
	abstract = {We continue a line of research initiated in [10,11]on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user.},
	language = {en},
	booktitle = {Theory of {Cryptography}},
	publisher = {Springer},
	author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
	editor = {Halevi, Shai and Rabin, Tal},
	year = {2006},
	keywords = {Laplace Distribution, Privacy Breach, Query Function, Semantic Security, True Answer},
	pages = {265--284},
}

@misc{noauthor_differentially_nodate,
	title = {Differentially {Private} {All}-{Pairs} {Shortest} {Path} {Distances}: {Improved} {Algorithms} and {Lower} {Bounds}},
	shorttitle = {Differentially {Private} {All}-{Pairs} {Shortest} {Path} {Distances}},
	url = {https://epubs.siam.org/doi/epdf/10.1137/1.9781611977554.ch184},
	language = {en},
	urldate = {2024-07-03},
	note = {ISBN: 9781611977554},
}

@article{sharan_lecture_nodate,
	title = {Lecture 4: {Concentration} {Inequalities}},
	language = {en},
	author = {Sharan, Vatsal and Wang, Ta-Yang},
}
